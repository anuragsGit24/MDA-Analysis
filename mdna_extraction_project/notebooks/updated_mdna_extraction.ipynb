{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db1347b",
   "metadata": {},
   "source": [
    "# Hybrid MD&A Extraction (Indian Annual Reports)\n",
    "\n",
    "This notebook extracts the **Management Discussion & Analysis (MD&A)** section from Indian Annual Report PDFs using a **hybrid parsing pipeline**:\n",
    "\n",
    "- **ToC/Index pages**: parsed with `pdfplumber` (layout-aware) to correctly bind section titles ↔ page numbers.\n",
    "- **Body pages**: extracted with `PyMuPDF` (`fitz`) for speed.\n",
    "\n",
    "Output: a CSV with columns `Filename, Company, Year, MD&A_Text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2129f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "import pathlib\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('mdna')\n",
    "\n",
    "# Notebook-safe project root resolution\n",
    "CWD = pathlib.Path.cwd().resolve()\n",
    "PROJECT_ROOT = CWD.parent if CWD.name.lower() == 'notebooks' else CWD\n",
    "\n",
    "PDF_ROOT = PROJECT_ROOT / 'data' / 'pdfs'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'output'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Use a different output name so this notebook doesn't overwrite older runs\n",
    "CSV_PATH = OUTPUT_DIR / 'mdna_extracted_hybrid.csv'\n",
    "\n",
    "# Tune these if needed\n",
    "TOC_SCAN_PAGES = 10\n",
    "HEADER_SCAN_PAGES_FALLBACK = None  # None = scan full document\n",
    "HEADER_REGION_MAX_Y = 120  # points (roughly top-of-page header)\n",
    "\n",
    "WHITESPACE_RE = re.compile(r'\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce54ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_text(s: str) -> str:\n",
    "    s = (s or '').replace('\\u00a0', ' ').replace('\\t', ' ')\n",
    "    s = WHITESPACE_RE.sub(' ', s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def _normalize_title(s: str) -> str:\n",
    "    s = _clean_text(s)\n",
    "    # normalize apostrophes / ampersands for matching\n",
    "    s = s.replace('’', \"'\")\n",
    "    s = s.replace('&', ' & ')\n",
    "    s = WHITESPACE_RE.sub(' ', s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def _extract_page_text_fitz_blocks(page: fitz.Page) -> str:\n",
    "    \"\"\"More stable than raw `get_text()` for multi-column pages.\"\"\"\n",
    "    blocks = page.get_text('blocks') or []\n",
    "    # block tuple: (x0, y0, x1, y1, text, block_no, block_type)\n",
    "    blocks_sorted = sorted(blocks, key=lambda b: (round(b[1], 1), round(b[0], 1)))\n",
    "    parts = []\n",
    "    for b in blocks_sorted:\n",
    "        txt = (b[4] or '').strip()\n",
    "        if txt:\n",
    "            parts.append(txt)\n",
    "    return _clean_text('\\n'.join(parts))\n",
    "\n",
    "\n",
    "def _header_text_fitz(page: fitz.Page, max_y: float = HEADER_REGION_MAX_Y) -> str:\n",
    "    blocks = page.get_text('blocks') or []\n",
    "    header_parts = []\n",
    "    for b in blocks:\n",
    "        y0 = b[1]\n",
    "        if y0 <= max_y:\n",
    "            t = (b[4] or '').strip()\n",
    "            if t:\n",
    "                header_parts.append(t)\n",
    "    return _clean_text(' '.join(header_parts))\n",
    "\n",
    "\n",
    "def _cluster_words_into_lines(words: list[dict], y_tolerance: float = 3.0) -> list[str]:\n",
    "    \"\"\"Group pdfplumber words into visual lines by `top` coordinate.\"\"\"\n",
    "    if not words:\n",
    "        return []\n",
    "\n",
    "    # Sort first so clustering is deterministic\n",
    "    words_sorted = sorted(words, key=lambda w: (w.get('top', 0.0), w.get('x0', 0.0)))\n",
    "    lines: list[list[dict]] = []\n",
    "    current: list[dict] = []\n",
    "    current_top: float | None = None\n",
    "\n",
    "    for w in words_sorted:\n",
    "        top = float(w.get('top', 0.0))\n",
    "        if current_top is None:\n",
    "            current_top = top\n",
    "            current = [w]\n",
    "            continue\n",
    "        if abs(top - current_top) <= y_tolerance:\n",
    "            current.append(w)\n",
    "        else:\n",
    "            lines.append(current)\n",
    "            current = [w]\n",
    "            current_top = top\n",
    "\n",
    "    if current:\n",
    "        lines.append(current)\n",
    "\n",
    "    out_lines: list[str] = []\n",
    "    for line_words in lines:\n",
    "        line_words_sorted = sorted(line_words, key=lambda w: float(w.get('x0', 0.0)))\n",
    "        txt = ' '.join((w.get('text') or '').strip() for w in line_words_sorted)\n",
    "        txt = _clean_text(txt)\n",
    "        if txt:\n",
    "            out_lines.append(txt)\n",
    "    return out_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1836451",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOC_HEADER_RE = re.compile(r\"\\b(contents|index)\\b\", re.IGNORECASE)\n",
    "\n",
    "# Matches: (Section Name)......(Page Number)  OR  (Section Name)    (Page Number)\n",
    "TOC_ENTRY_RE = re.compile(\n",
    "    r\"^(?P<title>.+?)\\s*(?:\\.{2,}|\\s{2,}|\\u00b7{2,}|\\-\\-\\-+)\\s*(?P<page>\\d{1,4})\\s*$\"\n",
    "    r\"|^(?P<title2>.+?)\\s+(?P<page2>\\d{1,4})\\s*$\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "NUM_ONLY_RE = re.compile(r\"^\\s*(\\d{1,4})\\s*$\")\n",
    "\n",
    "MDNA_TITLE_RE = re.compile(\n",
    "    r\"management\\s+discussion(?:s)?\\s*(?:and|&)\\s*analysis(?:\\s+report)?\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "TERMINATOR_TITLE_RE = re.compile(\n",
    "    r\"\\b(corporate\\s+governance|auditors?\\s*[’']?\\s*report|independent\\s+auditor|financial\\s+statements?|balance\\s+sheet|standalone\\s+financial|consolidated\\s+financial)\\b\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "\n",
    "def detect_toc_pages_hybrid(pdf_path: pathlib.Path, scan_pages: int = TOC_SCAN_PAGES) -> list[int]:\n",
    "    \"\"\"Hybrid switching logic: scan first N pages with fitz; if header contains CONTENTS/INDEX, mark as ToC page for pdfplumber parsing.\"\"\"\n",
    "    toc_pages: list[int] = []\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        n = min(scan_pages, doc.page_count)\n",
    "        for i in range(n):\n",
    "            page = doc.load_page(i)\n",
    "            # spec: header is first 1000 chars of extracted text (fast check)\n",
    "            head_sample = (page.get_text('text') or '')[:1000]\n",
    "            if TOC_HEADER_RE.search(head_sample):\n",
    "                toc_pages.append(i + 1)\n",
    "    return toc_pages\n",
    "\n",
    "\n",
    "def extract_toc_lines_pdfplumber(pdf_path: pathlib.Path, toc_pages: list[int]) -> list[str]:\n",
    "    \"\"\"Extract ToC lines (layout-aware) using pdfplumber for specified 1-based pages.\"\"\"\n",
    "    lines: list[str] = []\n",
    "    if not toc_pages:\n",
    "        return lines\n",
    "\n",
    "    with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "        for pno in toc_pages:\n",
    "            if pno < 1 or pno > len(pdf.pages):\n",
    "                continue\n",
    "            page = pdf.pages[pno - 1]\n",
    "            # words-based line clustering tends to preserve title↔page alignment better than plain extract_text()\n",
    "            words = page.extract_words(use_text_flow=True, keep_blank_chars=False) or []\n",
    "            page_lines = _cluster_words_into_lines(words, y_tolerance=3.0)\n",
    "            for ln in page_lines:\n",
    "                s = (ln or '').strip()\n",
    "                if s:\n",
    "                    lines.append(s)\n",
    "    return lines\n",
    "\n",
    "\n",
    "def parse_toc_entries(lines: Iterable[str], max_page: int) -> list[dict]:\n",
    "    \"\"\"Parse ToC lines into ordered entries with multiline-title merge (critical).\n",
    "\n",
    "    Rules (per spec):\n",
    "      - If a line has text but no page number, append it to the next line that has a page number.\n",
    "      - Also handle the variant where the next line is *only* a page number.\n",
    "    \"\"\"\n",
    "    entries: list[dict] = []\n",
    "    pending: list[str] = []\n",
    "\n",
    "    def _flush_pending_keep_last(max_keep: int = 3):\n",
    "        nonlocal pending\n",
    "        if len(pending) > max_keep:\n",
    "            pending = pending[-max_keep:]\n",
    "\n",
    "    for raw in lines:\n",
    "        s = _clean_text(raw)\n",
    "        if not s:\n",
    "            continue\n",
    "\n",
    "        # Skip ToC headings / boilerplate\n",
    "        if TOC_HEADER_RE.fullmatch(s) or re.fullmatch(r\"page\\s*(no\\.?|number)?\", s, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        m_num = NUM_ONLY_RE.match(s)\n",
    "        if m_num:\n",
    "            if pending:\n",
    "                page = int(m_num.group(1))\n",
    "                if 1 <= page <= max_page:\n",
    "                    title = _normalize_title(' '.join(pending))\n",
    "                    entries.append({\"title\": title, \"page\": page, \"raw\": f\"{title} -> {page}\"})\n",
    "                pending = []\n",
    "            continue\n",
    "\n",
    "        m = TOC_ENTRY_RE.match(s)\n",
    "        if m:\n",
    "            title = m.group('title') or m.group('title2') or ''\n",
    "            page_s = m.group('page') or m.group('page2') or ''\n",
    "            try:\n",
    "                page = int(page_s)\n",
    "            except ValueError:\n",
    "                page = None\n",
    "\n",
    "            if page is not None and (1 <= page <= max_page) and re.search(r\"[A-Za-z]\", title or ''):\n",
    "                title = _normalize_title(title)\n",
    "                if pending:\n",
    "                    title = _normalize_title(' '.join(pending + [title]))\n",
    "                    pending = []\n",
    "                entries.append({\"title\": title, \"page\": page, \"raw\": s})\n",
    "                continue\n",
    "\n",
    "        # No page number: buffer it to merge into the next ToC entry\n",
    "        if re.search(r\"[A-Za-z]\", s) and not TOC_HEADER_RE.search(s) and not re.search(r\"\\bpage\\b\", s, re.IGNORECASE):\n",
    "            pending.append(_normalize_title(s))\n",
    "            _flush_pending_keep_last()\n",
    "        else:\n",
    "            # non-title noise resets pending\n",
    "            pending = []\n",
    "\n",
    "    return entries\n",
    "\n",
    "\n",
    "def _is_mdna_title(title: str, company_folder: str | None = None) -> bool:\n",
    "    t = _normalize_title(title).lower()\n",
    "\n",
    "    # Core MD&A\n",
    "    if MDNA_TITLE_RE.search(t):\n",
    "        # Alchemist: do NOT treat Directors' Report as MD&A\n",
    "        if company_folder and company_folder.lower() in {'alcheimist', 'alchemist'}:\n",
    "            if 'director' in t and 'report' in t:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Amit Spinning edge: 'Board’s Report Including Management Discussions & Analysis Report'\n",
    "    if 'including' in t and 'board' in t and ('management' in t) and (('discussion' in t) or ('discussions' in t)) and ('analysis' in t):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def find_mdna_range_from_toc(pdf_path: pathlib.Path, company_folder: str | None = None) -> tuple[Optional[int], Optional[int], dict]:\n",
    "    \"\"\"Try ToC-driven MD&A start/end. Returns (start, end, debug).\"\"\"\n",
    "    debug = {\"toc_pages\": [], \"toc_entries\": [], \"mdna_entry\": None}\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        max_page = doc.page_count\n",
    "\n",
    "    toc_pages = detect_toc_pages_hybrid(pdf_path, scan_pages=TOC_SCAN_PAGES)\n",
    "    debug[\"toc_pages\"] = toc_pages\n",
    "    if not toc_pages:\n",
    "        return None, None, debug\n",
    "\n",
    "    toc_lines = extract_toc_lines_pdfplumber(pdf_path, toc_pages)\n",
    "    toc_entries = parse_toc_entries(toc_lines, max_page=max_page)\n",
    "    debug[\"toc_entries\"] = toc_entries\n",
    "    if not toc_entries:\n",
    "        return None, None, debug\n",
    "\n",
    "    mdna_idx = None\n",
    "    for i, ent in enumerate(toc_entries):\n",
    "        if _is_mdna_title(ent.get('title', ''), company_folder=company_folder):\n",
    "            mdna_idx = i\n",
    "            debug[\"mdna_entry\"] = ent\n",
    "            break\n",
    "\n",
    "    if mdna_idx is None:\n",
    "        return None, None, debug\n",
    "\n",
    "    start_page = int(toc_entries[mdna_idx]['page'])\n",
    "\n",
    "    # End page: next section in ToC (first entry after MD&A with higher page number)\n",
    "    next_page = None\n",
    "    for ent in toc_entries[mdna_idx + 1 :]:\n",
    "        p = ent.get('page')\n",
    "        if isinstance(p, int) and p > start_page:\n",
    "            next_page = p\n",
    "            break\n",
    "\n",
    "    end_page = (next_page - 1) if next_page else max_page\n",
    "    if end_page < start_page:\n",
    "        return None, None, debug\n",
    "\n",
    "    return start_page, end_page, debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21b7e283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mdna_range_by_header_fallback(pdf_path: pathlib.Path) -> tuple[Optional[int], Optional[int], dict]:\n",
    "    \"\"\"Fallback: search page headers for MD&A; derive end by next major section header.\"\"\"\n",
    "    debug = {\"start_hit_header\": None, \"end_hit_header\": None}\n",
    "\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        page_count = doc.page_count\n",
    "        scan_upto = page_count if HEADER_SCAN_PAGES_FALLBACK is None else min(page_count, int(HEADER_SCAN_PAGES_FALLBACK))\n",
    "\n",
    "        start = None\n",
    "        for i in range(scan_upto):\n",
    "            page = doc.load_page(i)\n",
    "            header = _header_text_fitz(page)\n",
    "            if MDNA_TITLE_RE.search(header):\n",
    "                start = i + 1\n",
    "                debug['start_hit_header'] = {'page': start, 'header': header}\n",
    "                break\n",
    "\n",
    "        if start is None:\n",
    "            return None, None, debug\n",
    "\n",
    "        end = page_count\n",
    "        for j in range(start, page_count):\n",
    "            page = doc.load_page(j)\n",
    "            header = _header_text_fitz(page)\n",
    "            if TERMINATOR_TITLE_RE.search(header):\n",
    "                end = j  # page before terminator (since j is 0-based, j==page_no-1)\n",
    "                debug['end_hit_header'] = {'page': j + 1, 'header': header}\n",
    "                break\n",
    "\n",
    "        if end < start:\n",
    "            return None, None, debug\n",
    "\n",
    "        return start, end, debug\n",
    "\n",
    "\n",
    "def extract_mdna_text(pdf_path: pathlib.Path, start_page: int, end_page: int) -> str:\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        start_i = max(0, start_page - 1)\n",
    "        end_i = min(doc.page_count - 1, end_page - 1)\n",
    "        parts = []\n",
    "        for i in range(start_i, end_i + 1):\n",
    "            page = doc.load_page(i)\n",
    "            parts.append(_extract_page_text_fitz_blocks(page))\n",
    "    return _clean_text('\\n'.join(p for p in parts if p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67060888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_name_from_folder(pdf_path: pathlib.Path) -> str:\n",
    "    return pdf_path.parent.name.replace('_', ' ').strip()\n",
    "\n",
    "\n",
    "def extract_financial_year_fitz(pdf_path: pathlib.Path, max_pages: int = 5) -> Optional[str]:\n",
    "    \"\"\"Best-effort year extraction from first pages.\"\"\"\n",
    "    year_patterns = [\n",
    "        re.compile(r\"\\b\\d{1,3}(?:st|nd|rd|th)\\s+Annual\\s+Report\\s+(\\d{4})\\s*[-–]\\s*(\\d{2,4})\\b\", re.IGNORECASE),\n",
    "        re.compile(r\"\\bAnnual\\s+Report\\s+(\\d{4})\\s*[-–]\\s*(\\d{2,4})\\b\", re.IGNORECASE),\n",
    "        re.compile(r\"\\bYear\\s+ended\\s+\\w+\\s+\\d{1,2},\\s+(\\d{4})\\b\", re.IGNORECASE),\n",
    "    ]\n",
    "\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        n = min(max_pages, doc.page_count)\n",
    "        for i in range(n):\n",
    "            page = doc.load_page(i)\n",
    "            txt = page.get_text('text') or ''\n",
    "            for pat in year_patterns:\n",
    "                m = pat.search(txt)\n",
    "                if not m:\n",
    "                    continue\n",
    "                groups = m.groups()\n",
    "                if len(groups) == 1:\n",
    "                    year = groups[0]\n",
    "                    prev = str(int(year) - 1)\n",
    "                    return f\"{prev}-{year[-2:]}\"\n",
    "                y1, y2 = groups\n",
    "                return f\"{y1}-{y2[-2:]}\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_pdf(pdf_path: pathlib.Path) -> dict:\n",
    "    company_folder = pdf_path.parent.name\n",
    "    company = extract_company_name_from_folder(pdf_path)\n",
    "    year = extract_financial_year_fitz(pdf_path)\n",
    "\n",
    "    start, end, toc_debug = find_mdna_range_from_toc(pdf_path, company_folder=company_folder)\n",
    "    method = 'toc' if (start and end) else None\n",
    "\n",
    "    if start is None or end is None:\n",
    "        start, end, hdr_debug = find_mdna_range_by_header_fallback(pdf_path)\n",
    "        if start and end:\n",
    "            method = 'header_fallback'\n",
    "        else:\n",
    "            hdr_debug = {}\n",
    "\n",
    "    if start is None or end is None:\n",
    "        return {\n",
    "            'Filename': pdf_path.name,\n",
    "            'Company': company,\n",
    "            'Year': year,\n",
    "            'MD&A_Text': '',\n",
    "            'StartPage': None,\n",
    "            'EndPage': None,\n",
    "            'Method': method or 'failed',\n",
    "        }\n",
    "\n",
    "    mdna_text = extract_mdna_text(pdf_path, start, end)\n",
    "\n",
    "    if company_folder.lower() in {'alcheimist', 'alchemist'}:\n",
    "        if re.search(r\"\\bdirectors\\s*[’']?\\s*report\\b\", mdna_text[:2000], re.IGNORECASE):\n",
    "            logger.warning('Alchemist: extracted text seems to include Directors\\' Report; trying header fallback range')\n",
    "            s2, e2, _ = find_mdna_range_by_header_fallback(pdf_path)\n",
    "            if s2 and e2:\n",
    "                mdna_text = extract_mdna_text(pdf_path, s2, e2)\n",
    "                start, end = s2, e2\n",
    "                method = 'header_fallback'\n",
    "\n",
    "    return {\n",
    "        'Filename': pdf_path.name,\n",
    "        'Company': company,\n",
    "        'Year': year,\n",
    "        'MD&A_Text': mdna_text,\n",
    "        'StartPage': start,\n",
    "        'EndPage': end,\n",
    "        'Method': method,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dad13e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-30 16:06:43,354 - INFO - Found 14 PDFs under C:\\Users\\LOQ\\Desktop\\SPJIMR\\mdna_extraction_project\\data\\pdfs\n",
      "Extracting MD&A: 100%|██████████| 14/14 [00:10<00:00,  1.36it/s]\n",
      "2025-12-30 16:06:53,623 - INFO - Wrote C:\\Users\\LOQ\\Desktop\\SPJIMR\\mdna_extraction_project\\output\\mdna_extracted_hybrid.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-30 16:06:43,354 - INFO - Found 14 PDFs under C:\\Users\\LOQ\\Desktop\\SPJIMR\\mdna_extraction_project\\data\\pdfs\n",
      "Extracting MD&A: 100%|██████████| 14/14 [00:10<00:00,  1.36it/s]\n",
      "2025-12-30 16:06:53,623 - INFO - Wrote C:\\Users\\LOQ\\Desktop\\SPJIMR\\mdna_extraction_project\\output\\mdna_extracted_hybrid.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Company</th>\n",
       "      <th>Year</th>\n",
       "      <th>MD&amp;A_Text</th>\n",
       "      <th>StartPage</th>\n",
       "      <th>EndPage</th>\n",
       "      <th>Method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5267070319.pdf</td>\n",
       "      <td>Alcheimist</td>\n",
       "      <td>2018-19</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67050526707.pdf</td>\n",
       "      <td>Alcheimist</td>\n",
       "      <td>2019-20</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5210700315.pdf</td>\n",
       "      <td>ALok</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5210700316.pdf</td>\n",
       "      <td>ALok</td>\n",
       "      <td>2015-16</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5210700317.pdf</td>\n",
       "      <td>ALok</td>\n",
       "      <td>2016-17</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5210700318.pdf</td>\n",
       "      <td>ALok</td>\n",
       "      <td>2017-18</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5210760315.pdf</td>\n",
       "      <td>Amit spinning</td>\n",
       "      <td>2014-15</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5210760316.pdf</td>\n",
       "      <td>Amit spinning</td>\n",
       "      <td>2015-16</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5210760317.pdf</td>\n",
       "      <td>Amit spinning</td>\n",
       "      <td>2016-17</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5210760318.pdf</td>\n",
       "      <td>Amit spinning</td>\n",
       "      <td>2017-18</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Filename        Company     Year MD&A_Text StartPage EndPage  Method\n",
       "0   5267070319.pdf     Alcheimist  2018-19                None    None  failed\n",
       "1  67050526707.pdf     Alcheimist  2019-20                None    None  failed\n",
       "2   5210700315.pdf           ALok     None                None    None  failed\n",
       "3   5210700316.pdf           ALok  2015-16                None    None  failed\n",
       "4   5210700317.pdf           ALok  2016-17                None    None  failed\n",
       "5   5210700318.pdf           ALok  2017-18                None    None  failed\n",
       "6   5210760315.pdf  Amit spinning  2014-15                None    None  failed\n",
       "7   5210760316.pdf  Amit spinning  2015-16                None    None  failed\n",
       "8   5210760317.pdf  Amit spinning  2016-17                None    None  failed\n",
       "9   5210760318.pdf  Amit spinning  2017-18                None    None  failed"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_paths = sorted(PDF_ROOT.rglob('*.pdf'))\n",
    "logger.info('Found %d PDFs under %s', len(pdf_paths), PDF_ROOT)\n",
    "\n",
    "out_cols = ['Filename', 'Company', 'Year', 'MD&A_Text']\n",
    "\n",
    "if not pdf_paths:\n",
    "    df = pd.DataFrame(columns=out_cols)\n",
    "    df.to_csv(CSV_PATH, index=False)\n",
    "    logger.warning('No PDFs found. Wrote empty CSV: %s', CSV_PATH)\n",
    "else:\n",
    "    results = []\n",
    "    for pdf_path in tqdm(pdf_paths, desc='Extracting MD&A'):\n",
    "        try:\n",
    "            results.append(process_pdf(pdf_path))\n",
    "        except Exception as e:\n",
    "            logger.exception('Failed on %s: %s', pdf_path, e)\n",
    "            results.append({\n",
    "                'Filename': pdf_path.name,\n",
    "                'Company': extract_company_name_from_folder(pdf_path),\n",
    "                'Year': extract_financial_year_fitz(pdf_path),\n",
    "                'MD&A_Text': '',\n",
    "                'StartPage': None,\n",
    "                'EndPage': None,\n",
    "                'Method': 'exception',\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df[out_cols].to_csv(CSV_PATH, index=False)\n",
    "    logger.info('Wrote %s', CSV_PATH)\n",
    "\n",
    "df[out_cols].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
