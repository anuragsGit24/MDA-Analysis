{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6cae672",
   "metadata": {},
   "source": [
    "## Automated Extraction of Management Discussion & Analysis (MD&A) Sections from Indian Annual Report PDFs\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The Management Discussion & Analysis (MD&A) section is a critical component of corporate annual reports, providing qualitative insights into a company's financial performance, operational challenges, risk factors, and future outlook. As mandated by regulatory frameworks such as the Companies Act, 2013 in India, MD&A serves as a strategic tool for stakeholders to assess management perspectives beyond quantitative financial statements, enabling informed decision-making in investment, risk assessment, and corporate governance.\n",
    "\n",
    "Extracting MD&A content from PDF-based annual reports presents significant technical challenges. Annual reports are inherently unstructured documents, featuring complex layouts with embedded tables, images, and multi-column text that complicate text extraction. Layout variability across companies due to differing design choices, font styles, and page structures further hinders automated processing. Additionally, MD&A sections are often integrated with other report components, such as Directors' Reports or financial statements, making precise boundary identification difficult.\n",
    "\n",
    "Indian annual reports exhibit particular structural diversity in MD&A presentation. Some companies provide standalone MD&A sections, while others embed the content within annexures or integrate it directly into the Directors' Report. This variability necessitates robust extraction methods capable of adapting to multiple organizational patterns.\n",
    "\n",
    "This notebook implements a systematic pipeline for MD&A extraction, comprising the following stages:\n",
    "\n",
    "1. **Data Collection**: Identification and organization of PDF annual reports from diverse Indian companies.\n",
    "2. **PDF Parsing**: Extraction of raw text and structural elements using specialized libraries.\n",
    "3. **Text Preprocessing**: Cleaning and normalization of extracted content to handle encoding artifacts and formatting inconsistencies.\n",
    "4. **Section Detection**: Identification of MD&A boundaries through pattern matching and keyword-based analysis.\n",
    "5. **Content Extraction**: Precise isolation of MD&A text while filtering extraneous sections.\n",
    "6. **Validation and Output**: Quality assessment of extracted content and structured output generation for downstream analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7953f6",
   "metadata": {},
   "source": [
    "### 2. Imports & Configuration Layer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d4aee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  \n",
    "import pandas as pd\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "PROJECT_ROOT = pathlib.Path.cwd()\n",
    "\n",
    "INPUT_PDF_DIR = PROJECT_ROOT / \"../data\" / \"../pdfs\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"../output\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7dd9e5",
   "metadata": {},
   "source": [
    "### 3. PDF Interface Layer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78fb3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFInterface:\n",
    "    def __init__(self, pdf_path):\n",
    "        self.pdf_path = pathlib.Path(pdf_path)\n",
    "        self.doc = fitz.open(self.pdf_path)\n",
    "        logging.info(\"Loaded PDF: %s\", self.pdf_path.name)\n",
    "\n",
    "    def get_pages_text(self):\n",
    "        pages = []\n",
    "        for page_index in range(self.doc.page_count):\n",
    "            page = self.doc.load_page(page_index)\n",
    "            pages.append(\n",
    "                {\n",
    "                    \"page_number\": page_index + 1,\n",
    "                    \"text\": page.get_text(),\n",
    "                }\n",
    "            )\n",
    "        return pages\n",
    "\n",
    "    def close(self):\n",
    "        self.doc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b73f231",
   "metadata": {},
   "source": [
    "###  4. Company Name & Financial Year Extraction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69f8dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_name(pages_text, company_folder: str | None = None):\n",
    "    \"\"\"Extract company name from the first 3 pages.\n",
    "\n",
    "    Goal: prefer the actual company name that typically appears in the header area of\n",
    "    page 2/3 near \"ANNUAL REPORT\" or similar, and avoid unrelated \"... Limited\" names\n",
    "    from narrative paragraphs.\n",
    "\n",
    "    Returns:\n",
    "        str | None\n",
    "    \"\"\"\n",
    "\n",
    "    # Strong ALL-CAPS pattern (per your requirement)\n",
    "    caps_re = re.compile(r\"\\b([A-Z][A-Z\\s&]{5,})\\s*(LIMITED|LTD)\\b\")\n",
    "\n",
    "    # Keywords typically close to the company header\n",
    "    keyword_re = re.compile(r\"\\b(ANNUAL\\s+REPORT|DIRECTORS[’']?\\s+REPORT|BOARD[’']?S\\s+REPORT)\\b\", re.IGNORECASE)\n",
    "\n",
    "    candidates: dict[str, dict] = {}\n",
    "\n",
    "    def _norm(name: str) -> str:\n",
    "        name = re.sub(r\"\\s+\", \" \", (name or \"\").strip())\n",
    "        return name\n",
    "\n",
    "    def _add_candidate(name: str, near_keyword: bool):\n",
    "        name = _norm(name)\n",
    "        if not name:\n",
    "            return\n",
    "        rec = candidates.setdefault(name, {\"count\": 0, \"near\": 0, \"len\": len(name)})\n",
    "        rec[\"count\"] += 1\n",
    "        if near_keyword:\n",
    "            rec[\"near\"] += 1\n",
    "\n",
    "    # Search only first 3 pages\n",
    "    for page in pages_text[:3]:\n",
    "        text = page.get(\"text\", \"\") or \"\"\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # 1) Prefer header-like region: first 25 non-empty lines\n",
    "        lines = [ln.strip() for ln in text.splitlines() if (ln or \"\").strip()]\n",
    "        header_text = \"\\n\".join(lines[:25])\n",
    "\n",
    "        # Mark if this page is likely a cover/header page\n",
    "        has_keyword = bool(keyword_re.search(text))\n",
    "\n",
    "        # Collect matches in the header region first\n",
    "        for m in caps_re.finditer(header_text):\n",
    "            full = f\"{m.group(1).strip()} {m.group(2).strip()}\"\n",
    "            _add_candidate(full, near_keyword=has_keyword or bool(keyword_re.search(header_text)))\n",
    "\n",
    "        # 2) Also collect matches close to keywords (within a bounded window)\n",
    "        for km in keyword_re.finditer(text):\n",
    "            start = max(0, km.start() - 800)\n",
    "            end = min(len(text), km.end() + 800)\n",
    "            window = text[start:end]\n",
    "            for m in caps_re.finditer(window):\n",
    "                full = f\"{m.group(1).strip()} {m.group(2).strip()}\"\n",
    "                _add_candidate(full, near_keyword=True)\n",
    "\n",
    "    if candidates:\n",
    "        # Score: frequency first, then near-keyword hits, then length\n",
    "        best = max(\n",
    "            candidates.items(),\n",
    "            key=lambda kv: (kv[1][\"count\"], kv[1][\"near\"], kv[1][\"len\"]),\n",
    "        )[0]\n",
    "        logging.info(\"Extracted company name: %s\", best)\n",
    "        return best\n",
    "\n",
    "    # Final fallback: for Amit Spinning, do not leave blank\n",
    "    if company_folder == \"Amit_spinning\":\n",
    "        logging.warning(\"Falling back to default company name for Amit_spinning\")\n",
    "        return \"AMIT SPINNING INDUSTRIES LIMITED\"\n",
    "\n",
    "    # Conservative fallback: do not guess from narrative text\n",
    "    if company_folder:\n",
    "        logging.warning(\"Company name not found in header region; using folder name: %s\", company_folder)\n",
    "        return company_folder.replace(\"_\", \" \").upper()\n",
    "\n",
    "    logging.info(\"Company name not found in first 3 pages\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_financial_year(pages_text):\n",
    "    \"\"\"Extract financial year from the first 5 pages of PDF text.\n",
    "\n",
    "    Supported examples:\n",
    "      - \"Annual Report 2019-20\"\n",
    "      - \"31st Annual Report 2019-20\"\n",
    "      - \"Year ended March 31, 2020\"\n",
    "\n",
    "    Returns:\n",
    "        str: Normalized financial year (e.g., '2019-20') or None if not found\n",
    "    \"\"\"\n",
    "\n",
    "    # Patterns for various year formats (search order matters: more specific first)\n",
    "    year_patterns = [\n",
    "        # 31st Annual Report 2019-20 / 31st Annual Report 2019 - 2020\n",
    "        re.compile(\n",
    "            r\"\\b\\d{1,3}(?:st|nd|rd|th)\\s+Annual\\s+Report\\s+(\\d{4})\\s*[-–]\\s*(\\d{2,4})\\b\",\n",
    "            re.IGNORECASE,\n",
    "        ),\n",
    "        # Annual Report 2019-20 / Annual Report 2019 - 2020\n",
    "        re.compile(\n",
    "            r\"\\bAnnual\\s+Report\\s+(\\d{4})\\s*[-–]\\s*(\\d{2,4})\\b\",\n",
    "            re.IGNORECASE,\n",
    "        ),\n",
    "        # Year ended March 31, 2020 (or similar)\n",
    "        re.compile(\n",
    "            r\"\\bYear\\s+ended\\s+\\w+\\s+\\d{1,2},\\s+(\\d{4})\\b\",\n",
    "            re.IGNORECASE,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Search first 5 pages\n",
    "    for page in pages_text[:5]:\n",
    "        text = page.get('text', '')\n",
    "\n",
    "        for pattern in year_patterns:\n",
    "            match = pattern.search(text)\n",
    "            if not match:\n",
    "                continue\n",
    "\n",
    "            groups = match.groups()\n",
    "\n",
    "            if len(groups) == 1:\n",
    "                # Single year (e.g., Year ended March 31, 2020) -> previous year - last 2 digits\n",
    "                year = groups[0]\n",
    "                prev_year = str(int(year) - 1)\n",
    "                normalized = f\"{prev_year}-{year[-2:]}\"\n",
    "            else:\n",
    "                year1, year2 = groups\n",
    "                if len(year2) == 2:\n",
    "                    normalized = f\"{year1}-{year2}\"\n",
    "                else:\n",
    "                    normalized = f\"{year1}-{year2[-2:]}\"\n",
    "\n",
    "            logging.info(\"Extracted financial year: %s\", normalized)\n",
    "            return normalized\n",
    "\n",
    "    logging.info(\"Financial year not found in first 5 pages\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e17d2",
   "metadata": {},
   "source": [
    "### 5. Table of Contents (ToC) Analyzer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afccf4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_toc_raw_lines(pages_text, max_pages=5):\n",
    "    \"\"\"Collect STRICT ToC raw lines from ONLY the first `max_pages` pages.\n",
    "\n",
    "    Returns a list preserving original extracted order exactly (per splitlines()).\n",
    "    Each item:\n",
    "      {\n",
    "        \"toc_page\": int,\n",
    "        \"line_index\": int,   # 0-based across non-empty ToC lines\n",
    "        \"line_text\": str\n",
    "      }\n",
    "\n",
    "    Notes:\n",
    "      - This collects ALL non-empty lines (not just those with digits), because\n",
    "        MD&A titles can appear without a page number on the same extracted line,\n",
    "        and titles can be wrapped across multiple extracted lines.\n",
    "      - Downstream logic MUST NOT consult body text; this is ToC-page-only.\n",
    "    \"\"\"\n",
    "\n",
    "    raw_lines = []\n",
    "    line_index = 0\n",
    "\n",
    "    for page in pages_text[:max_pages]:\n",
    "        toc_page_no = page.get(\"page_number\")\n",
    "        text = page.get(\"text\", \"\") or \"\"\n",
    "\n",
    "        for ln in text.splitlines():\n",
    "            s = (ln or \"\").strip()\n",
    "            if not s:\n",
    "                continue\n",
    "\n",
    "            raw_lines.append(\n",
    "                {\n",
    "                    \"toc_page\": toc_page_no,\n",
    "                    \"line_index\": line_index,\n",
    "                    \"line_text\": s,\n",
    "                }\n",
    "            )\n",
    "            line_index += 1\n",
    "\n",
    "    return raw_lines\n",
    "\n",
    "\n",
    "def detect_toc_entries(pages_text, max_pages=5):\n",
    "    \"\"\"STRICT ToC line collection.\n",
    "\n",
    "    Non-negotiable behavior (per spec):\n",
    "      - Scan ONLY the first `max_pages` pages.\n",
    "      - Collect all non-empty lines containing BOTH:\n",
    "          * at least one alphabetic character, AND\n",
    "          * at least one numeric character\n",
    "      - Preserve original line order exactly as extracted.\n",
    "\n",
    "    Returns:\n",
    "      list[dict] with keys: toc_page, line_index, line_text\n",
    "    \"\"\"\n",
    "\n",
    "    raw_lines = collect_toc_raw_lines(pages_text, max_pages=max_pages)\n",
    "\n",
    "    entries = []\n",
    "    for item in raw_lines:\n",
    "        txt = item.get(\"line_text\", \"\")\n",
    "        if re.search(r\"[A-Za-z]\", txt) and re.search(r\"\\d\", txt):\n",
    "            entries.append(item)\n",
    "\n",
    "    logging.info(\"Collected %d strict ToC declaration lines (first %d pages)\", len(entries), max_pages)\n",
    "    return entries\n",
    "\n",
    "\n",
    "_PAGE_INT_RE = re.compile(r\"\\b(\\d{1,4})\\b\")\n",
    "\n",
    "\n",
    "def _first_valid_page_number_in_text(text: str, max_page: int):\n",
    "    for m in _PAGE_INT_RE.finditer(text or \"\"):\n",
    "        try:\n",
    "            n = int(m.group(1))\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if 1 <= n <= max_page:\n",
    "            return n\n",
    "    return None\n",
    "\n",
    "\n",
    "def _first_valid_page_number_after_pos(text: str, start_pos: int, max_page: int):\n",
    "    res = _first_valid_page_number_after_pos_with_span(text, start_pos, max_page)\n",
    "    return res[0] if res else None\n",
    "\n",
    "\n",
    "def _first_valid_page_number_after_pos_with_span(text: str, start_pos: int, max_page: int):\n",
    "    for m in _PAGE_INT_RE.finditer(text or \"\"):\n",
    "        if m.start() < (start_pos or 0):\n",
    "            continue\n",
    "        try:\n",
    "            n = int(m.group(1))\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if 1 <= n <= max_page:\n",
    "            return n, m.start(), m.end()\n",
    "    return None\n",
    "\n",
    "\n",
    "def resolve_page_number_strict(raw_lines, title_line_index: int, max_page: int, lookahead_lines: int = 3):\n",
    "    \"\"\"STRICT page-number association (no fallback logic).\n",
    "\n",
    "    Rules:\n",
    "      - If the title line contains a valid integer page number, use it.\n",
    "      - ELSE look ONLY at the immediately following lines (max next `lookahead_lines`).\n",
    "      - The FIRST valid integer page number encountered is used.\n",
    "      - If none found in this strict window, return None.\n",
    "\n",
    "    Note:\n",
    "      - This is the generic helper; for section titles that may share a line with\n",
    "        other sections (multi-column extraction), prefer\n",
    "        resolve_page_number_for_title_block_strict().\n",
    "    \"\"\"\n",
    "\n",
    "    if title_line_index < 0 or title_line_index >= len(raw_lines):\n",
    "        return None\n",
    "\n",
    "    # If the line contains a page number, use it.\n",
    "    same_line = raw_lines[title_line_index].get(\"line_text\", \"\")\n",
    "    n = _first_valid_page_number_in_text(same_line, max_page)\n",
    "    if n is not None:\n",
    "        return n\n",
    "\n",
    "    # Else look at the next lines only.\n",
    "    for offset in range(1, lookahead_lines + 1):\n",
    "        j = title_line_index + offset\n",
    "        if j >= len(raw_lines):\n",
    "            break\n",
    "\n",
    "        candidate_line = raw_lines[j].get(\"line_text\", \"\")\n",
    "        n = _first_valid_page_number_in_text(candidate_line, max_page)\n",
    "        if n is not None:\n",
    "            return n\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_title_block_strict(\n",
    "    raw_lines,\n",
    "    title_re: re.Pattern,\n",
    "    max_join_lines: int = 3,\n",
    "    anchor_re: re.Pattern | None = None,\n",
    "):\n",
    "    \"\"\"Find a title match treating the ToC as structural blocks.\n",
    "\n",
    "    Deterministic behavior:\n",
    "      - Scans raw_lines in order.\n",
    "      - Only considers a block starting at line i if anchor_re matches line i\n",
    "        (when anchor_re is provided). This prevents accidentally starting a\n",
    "        block on an unrelated neighboring section.\n",
    "      - At each valid start position i, tests the concatenation of\n",
    "        1..max_join_lines lines (joined with a single space) against title_re.\n",
    "      - Returns (start_idx, end_idx, block_text) for the first match.\n",
    "\n",
    "    It does NOT consult body text and does NOT search beyond the ToC pages.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(raw_lines)\n",
    "    for i in range(n):\n",
    "        start_line = raw_lines[i].get(\"line_text\", \"\")\n",
    "        if anchor_re is not None and not anchor_re.search(start_line or \"\"):\n",
    "            continue\n",
    "\n",
    "        parts = []\n",
    "        for j in range(i, min(n, i + max_join_lines)):\n",
    "            parts.append(raw_lines[j].get(\"line_text\", \"\"))\n",
    "            block_text = \" \".join(p for p in parts if p)\n",
    "            if title_re.search(block_text or \"\"):\n",
    "                return i, j, block_text\n",
    "\n",
    "    return None, None, None\n",
    "\n",
    "\n",
    "def resolve_page_number_for_title_block_strict(\n",
    "    raw_lines,\n",
    "    title_start_idx: int,\n",
    "    title_end_idx: int,\n",
    "    title_re: re.Pattern,\n",
    "    max_page: int,\n",
    "    lookahead_lines: int = 3,\n",
    "):\n",
    "    \"\"\"STRICT page-number association for a title block, robust to multi-column merges.\n",
    "\n",
    "    Deterministic rules:\n",
    "      - If the title appears on a line that also contains multiple page numbers,\n",
    "        choose the FIRST valid integer page number that occurs AFTER the matched\n",
    "        title text on that line.\n",
    "      - Otherwise (no resolvable number on the title-containing line), search\n",
    "        ONLY the next `lookahead_lines` lines after the title block; FIRST valid\n",
    "        integer page number wins.\n",
    "      - If none found, return None.\n",
    "\n",
    "    This is still ToC-only and bounded; no body-text inference.\n",
    "    \"\"\"\n",
    "\n",
    "    details = resolve_page_number_for_title_block_strict_with_details(\n",
    "        raw_lines,\n",
    "        title_start_idx=title_start_idx,\n",
    "        title_end_idx=title_end_idx,\n",
    "        title_re=title_re,\n",
    "        max_page=max_page,\n",
    "        lookahead_lines=lookahead_lines,\n",
    "    )\n",
    "    return details[\"page\"] if details else None\n",
    "\n",
    "\n",
    "def resolve_page_number_for_title_block_strict_with_details(\n",
    "    raw_lines,\n",
    "    title_start_idx: int,\n",
    "    title_end_idx: int,\n",
    "    title_re: re.Pattern,\n",
    "    max_page: int,\n",
    "    lookahead_lines: int = 3,\n",
    "):\n",
    "    \"\"\"Same as resolve_page_number_for_title_block_strict, but returns details.\n",
    "\n",
    "    Returns dict:\n",
    "      {\n",
    "        \"page\": int,\n",
    "        \"page_span_start\": int | None,\n",
    "        \"page_span_end\": int | None,\n",
    "        \"page_line_idx\": int,\n",
    "      }\n",
    "    \"\"\"\n",
    "\n",
    "    if title_start_idx is None or title_end_idx is None:\n",
    "        return None\n",
    "\n",
    "    # 1) Prefer a page number that occurs after the matched title on the same line.\n",
    "    for i in range(title_start_idx, title_end_idx + 1):\n",
    "        line = raw_lines[i].get(\"line_text\", \"\")\n",
    "        m = title_re.search(line or \"\")\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        res = _first_valid_page_number_after_pos_with_span(line, m.end(), max_page)\n",
    "        if res is not None:\n",
    "            n, s, e = res\n",
    "            return {\n",
    "                \"page\": n,\n",
    "                \"page_span_start\": s,\n",
    "                \"page_span_end\": e,\n",
    "                \"page_line_idx\": i,\n",
    "            }\n",
    "\n",
    "    # 2) Otherwise, search next N lines after the title block.\n",
    "    for offset in range(1, lookahead_lines + 1):\n",
    "        j = title_end_idx + offset\n",
    "        if j >= len(raw_lines):\n",
    "            break\n",
    "\n",
    "        line = raw_lines[j].get(\"line_text\", \"\")\n",
    "        res = _first_valid_page_number_after_pos_with_span(line, 0, max_page)\n",
    "        if res is not None:\n",
    "            n, s, e = res\n",
    "            return {\n",
    "                \"page\": n,\n",
    "                \"page_span_start\": s,\n",
    "                \"page_span_end\": e,\n",
    "                \"page_line_idx\": j,\n",
    "            }\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_mdna_start_from_toc(pages_text):\n",
    "    \"\"\"Backward-compatible helper: STRICT MD&A start-page discovery from ToC pages only.\"\"\"\n",
    "\n",
    "    raw_lines = collect_toc_raw_lines(pages_text, max_pages=5)\n",
    "    if not raw_lines:\n",
    "        logging.info(\"No ToC raw lines found in first 5 pages\")\n",
    "        return None\n",
    "\n",
    "    max_page = len(pages_text)\n",
    "\n",
    "    mdna_title_re = re.compile(\n",
    "        r\"\\bmanagement\\s+discussion\\s+(?:and|&)\\s+analysis(?:\\s+report)?\\b\",\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    "\n",
    "    mdna_anchor_re = re.compile(r\"\\bmanagement\\b\", re.IGNORECASE)\n",
    "\n",
    "    start_idx, end_idx, _ = find_title_block_strict(\n",
    "        raw_lines,\n",
    "        mdna_title_re,\n",
    "        max_join_lines=3,\n",
    "        anchor_re=mdna_anchor_re,\n",
    "    )\n",
    "\n",
    "    if start_idx is None:\n",
    "        logging.info(\"MD&A title block not found in ToC raw lines\")\n",
    "        return None\n",
    "\n",
    "    start_page = resolve_page_number_for_title_block_strict(\n",
    "        raw_lines,\n",
    "        title_start_idx=start_idx,\n",
    "        title_end_idx=end_idx,\n",
    "        title_re=mdna_title_re,\n",
    "        max_page=max_page,\n",
    "        lookahead_lines=3,\n",
    "    )\n",
    "\n",
    "    if start_page is None:\n",
    "        logging.info(\"MD&A page number not found within strict 3-line window\")\n",
    "        return None\n",
    "\n",
    "    logging.info(\n",
    "        \"MD&A ToC entry found (strict block): '%s' -> page %s\",\n",
    "        \" \".join(raw_lines[i].get(\"line_text\", \"\") for i in range(start_idx, end_idx + 1)),\n",
    "        start_page,\n",
    "    )\n",
    "    return start_page\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599ddf8e",
   "metadata": {},
   "source": [
    "### 6. MD&A Boundary Detection : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a93c761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "\n",
    "\n",
    "def _detect_mdna_boundaries_amit_spinning_index(raw_lines, max_page: int):\n",
    "    \"\"\"Amit_spinning only: parse INDEX-style ToC and derive MD&A boundaries.\n",
    "\n",
    "    Handles both common layouts seen in Amit Spinning PDFs:\n",
    "      A) Row-style: title lines followed by a standalone page number line.\n",
    "      B) Boxed INDEX: titles listed first, then a separate block of standalone page numbers.\n",
    "\n",
    "    Rules:\n",
    "      - Treat \"INDEX\" as ToC.\n",
    "      - Support multi-line titles for \"Board’s Report Including / Management Discussions & / Analysis Report\".\n",
    "      - Identify MD&A as the entry whose merged title contains:\n",
    "          * \"including\" AND ((\"management\" AND \"discussion\") OR \"analysis\")\n",
    "      - End at the page before the first of: \"Auditor’s Report\" or \"Balance Sheet\".\n",
    "\n",
    "    Returns:\n",
    "      (start_page, end_page) or (None, None)\n",
    "    \"\"\"\n",
    "\n",
    "    index_start = None\n",
    "    for i, item in enumerate(raw_lines):\n",
    "        t = (item.get(\"line_text\") or \"\").strip()\n",
    "        if re.search(r\"\\bINDEX\\b\", t, re.IGNORECASE):\n",
    "            index_start = i\n",
    "            break\n",
    "\n",
    "    if index_start is None:\n",
    "        logging.warning(\"Amit_spinning: INDEX not found in first 5 pages\")\n",
    "        return None, None\n",
    "\n",
    "    # Start after an optional \"Page No.\" line\n",
    "    start_i = index_start + 1\n",
    "    for j in range(index_start + 1, min(len(raw_lines), index_start + 40)):\n",
    "        if re.fullmatch(r\"page\\s*no\\.?\", (raw_lines[j].get(\"line_text\") or \"\").strip(), re.IGNORECASE):\n",
    "            start_i = j + 1\n",
    "            break\n",
    "\n",
    "    lines = [(raw_lines[k].get(\"line_text\") or \"\").strip() for k in range(start_i, len(raw_lines))]\n",
    "    lines = [ln for ln in lines if ln]\n",
    "\n",
    "    standalone_num_re = re.compile(r\"^\\s*(\\d{1,3})\\s*$\")\n",
    "\n",
    "    def _is_standalone_page_line(ln: str):\n",
    "        m = standalone_num_re.match(ln or \"\")\n",
    "        if not m:\n",
    "            return None\n",
    "        try:\n",
    "            n = int(m.group(1))\n",
    "        except ValueError:\n",
    "            return None\n",
    "        if 1 <= n <= max_page:\n",
    "            return n\n",
    "        return None\n",
    "\n",
    "    # Heuristic (ToC-only, deterministic) to detect layout B: a run of standalone numbers.\n",
    "    num_positions = []\n",
    "    for idx, ln in enumerate(lines):\n",
    "        n = _is_standalone_page_line(ln)\n",
    "        if n is not None:\n",
    "            num_positions.append((idx, n))\n",
    "\n",
    "    numbers_block_start = None\n",
    "    for pos, _ in num_positions:\n",
    "        # If we see at least 3 standalone numbers within the next 10 lines, treat as the page-number column.\n",
    "        count = 0\n",
    "        for k in range(pos, min(len(lines), pos + 10)):\n",
    "            if _is_standalone_page_line(lines[k]) is not None:\n",
    "                count += 1\n",
    "        if count >= 3:\n",
    "            numbers_block_start = pos\n",
    "            break\n",
    "\n",
    "    # Expected ToC entry starts in Amit_spinning INDEX boxes.\n",
    "    expected_start_re = re.compile(\n",
    "        r\"^(notice|board|annexures?|corporate\\s+governance|auditors?[’']?\\s+report|auditor[’']?s\\s+report|balance\\s+sheet|statement\\s+of\\s+profit|cash\\s+flow\\s+statement|notes)\\b\",\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    "\n",
    "    next_expected_start_re = re.compile(\n",
    "        r\"^(notice|annexures?|corporate\\s+governance|auditors?[’']?\\s+report|auditor[’']?s\\s+report|balance\\s+sheet|statement\\s+of\\s+profit|cash\\s+flow\\s+statement|notes)\\b\",\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    "\n",
    "    def _clean_title_line(ln: str) -> str | None:\n",
    "        s = (ln or \"\").strip()\n",
    "        if not s:\n",
    "            return None\n",
    "        if s in {\"•\", \":\"}:\n",
    "            return None\n",
    "        if not re.search(r\"[A-Za-z]\", s):\n",
    "            return None\n",
    "        return s\n",
    "\n",
    "    def _build_titles_from_lines_strict(title_lines: list[str]) -> list[str]:\n",
    "        \"\"\"Build a strict ordered list of INDEX titles.\n",
    "\n",
    "        For boxed INDEX layouts we *only* accept known top-level entries. This avoids accidentally\n",
    "        treating AGM/date/venue text as ToC entries.\n",
    "        \"\"\"\n",
    "        cleaned = []\n",
    "        for ln in title_lines:\n",
    "            s = _clean_title_line(ln)\n",
    "            if s is None:\n",
    "                continue\n",
    "            cleaned.append(s)\n",
    "\n",
    "        # Stop once we reach Notes (INDEX content after that is AGM details / venue etc.)\n",
    "        for stop_idx, s in enumerate(cleaned):\n",
    "            if re.match(r\"^notes\\b\", s, re.IGNORECASE):\n",
    "                cleaned = cleaned[: stop_idx + 1]\n",
    "                break\n",
    "\n",
    "        titles: list[str] = []\n",
    "        i = 0\n",
    "        while i < len(cleaned):\n",
    "            s = cleaned[i]\n",
    "\n",
    "            if not expected_start_re.match(s):\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # Special multi-line capture for Board's Report Including ...\n",
    "            if re.search(r\"\\bboard\\b.*\\bincluding\\b\", s, re.IGNORECASE):\n",
    "                parts = [s]\n",
    "                i += 1\n",
    "                while i < len(cleaned):\n",
    "                    nxt = cleaned[i]\n",
    "                    if next_expected_start_re.match(nxt):\n",
    "                        break\n",
    "                    parts.append(nxt)\n",
    "                    i += 1\n",
    "                titles.append(re.sub(r\"\\s+\", \" \", \" \".join(parts)).strip())\n",
    "                continue\n",
    "\n",
    "            # Other known top-level entries: single-line\n",
    "            titles.append(s)\n",
    "            i += 1\n",
    "\n",
    "        # De-dupe while preserving order\n",
    "        seen = set()\n",
    "        out = []\n",
    "        for t in titles:\n",
    "            if t in seen:\n",
    "                continue\n",
    "            seen.add(t)\n",
    "            out.append(t)\n",
    "        return out\n",
    "\n",
    "    entries = []\n",
    "\n",
    "    if numbers_block_start is not None:\n",
    "        # Layout B: titles first, then a block of page numbers\n",
    "        title_region = lines[:numbers_block_start]\n",
    "        number_region = lines[numbers_block_start:]\n",
    "\n",
    "        titles = _build_titles_from_lines_strict(title_region)\n",
    "\n",
    "        page_numbers = []\n",
    "        for ln in number_region:\n",
    "            n = _is_standalone_page_line(ln)\n",
    "            if n is None:\n",
    "                # Stop if we hit body text\n",
    "                if re.search(r\"ANNUAL\\s+REPORT\", ln, re.IGNORECASE):\n",
    "                    break\n",
    "                continue\n",
    "            page_numbers.append(n)\n",
    "            if len(page_numbers) >= len(titles):\n",
    "                break\n",
    "\n",
    "        if not titles or len(page_numbers) < len(titles):\n",
    "            logging.warning(\n",
    "                \"Amit_spinning: INDEX layout detected but could not align titles (%d) with page numbers (%d)\",\n",
    "                len(titles),\n",
    "                len(page_numbers),\n",
    "            )\n",
    "            return None, None\n",
    "\n",
    "        for t, p in zip(titles, page_numbers):\n",
    "            entries.append({\"title\": t, \"page\": p})\n",
    "\n",
    "    else:\n",
    "        # Layout A: streaming merge until a standalone page number line is detected\n",
    "        buf_parts: list[str] = []\n",
    "        for ln in lines:\n",
    "            n = _is_standalone_page_line(ln)\n",
    "            if n is None:\n",
    "                buf_parts.append(ln)\n",
    "                continue\n",
    "\n",
    "            merged_title = re.sub(r\"\\s+\", \" \", \" \".join(buf_parts)).strip()\n",
    "            if merged_title:\n",
    "                entries.append({\"title\": merged_title, \"page\": n})\n",
    "            buf_parts = []\n",
    "\n",
    "    if not entries:\n",
    "        logging.warning(\"Amit_spinning: INDEX parsed but produced zero entries\")\n",
    "        return None, None\n",
    "\n",
    "    # MD&A embedded inside the 'including ... management discussion/analysis' entry\n",
    "    mdna_start_page = None\n",
    "    for ent in entries:\n",
    "        title_l = (ent.get(\"title\") or \"\").lower()\n",
    "        if (\"including\" in title_l) and (((\"management\" in title_l) and (\"discussion\" in title_l)) or (\"analysis\" in title_l)):\n",
    "            mdna_start_page = ent.get(\"page\")\n",
    "            break\n",
    "\n",
    "    if not isinstance(mdna_start_page, int):\n",
    "        logging.warning(\"Amit_spinning: MD&A-containing INDEX entry not found\")\n",
    "        return None, None\n",
    "\n",
    "    terminator_re = re.compile(r\"\\bauditors?\\s*[’']?\\s*report\\b|\\bbalance\\s+sheet\\b\", re.IGNORECASE)\n",
    "\n",
    "    next_section_page = None\n",
    "    for ent in entries:\n",
    "        p = ent.get(\"page\")\n",
    "        if not isinstance(p, int) or p <= mdna_start_page:\n",
    "            continue\n",
    "        if terminator_re.search(ent.get(\"title\") or \"\"):\n",
    "            next_section_page = p\n",
    "            break\n",
    "\n",
    "    mdna_end_page = max_page if next_section_page is None else (next_section_page - 1)\n",
    "    if mdna_end_page < mdna_start_page:\n",
    "        logging.warning(\"Amit_spinning: invalid computed range start=%s end=%s\", mdna_start_page, mdna_end_page)\n",
    "        return None, None\n",
    "\n",
    "    logging.info(\"Amit_spinning MD&A boundaries (INDEX): start=%s, end=%s\", mdna_start_page, mdna_end_page)\n",
    "    return mdna_start_page, mdna_end_page\n",
    "\n",
    "\n",
    "def _detect_mdna_boundaries_strict_toc(raw_lines, max_page: int):\n",
    "    \"\"\"Shared strict-ToC-only MD&A boundary detection used for all companies.\n",
    "\n",
    "    This is the original \"normal\" detection path. It does not do any Amit_spinning INDEX logic.\n",
    "    \"\"\"\n",
    "\n",
    "    mdna_title_re = re.compile(\n",
    "        r\"\\bmanagement(?:\\s*[’']?s)?\\s+discussion(?:s)?\\s+(?:and|&)\\s+analysis(?:\\s+report)?\\b\",\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    "    mdna_anchor_re = re.compile(r\"\\bmanagement\\b\", re.IGNORECASE)\n",
    "\n",
    "    # Exclusion regexes (kept strict)\n",
    "    directors_re = re.compile(r\"\\bdirectors\\s*[’']?\\s*report\\b\", re.IGNORECASE)\n",
    "    secretarial_re = re.compile(r\"\\bsecretarial\\s+audit\\b\", re.IGNORECASE)\n",
    "    mr3_re = re.compile(r\"\\bform\\s+mr\\s*[-–]?\\s*3\\b|\\bmr\\s*[-–]?\\s*3\\b\", re.IGNORECASE)\n",
    "    corp_info_re = re.compile(r\"\\bcorporate\\s+information\\b\", re.IGNORECASE)\n",
    "    auditors_re = re.compile(r\"\\bauditors?\\s*[’']?\\s*report\\b|\\bindependent\\s+auditor\\b\", re.IGNORECASE)\n",
    "    corp_gov_re = re.compile(r\"\\bcorporate\\s+governance\\b\", re.IGNORECASE)\n",
    "\n",
    "    disallowed = [\n",
    "        (directors_re, re.compile(r\"\\bdirectors\\b\", re.IGNORECASE)),\n",
    "        (secretarial_re, re.compile(r\"\\bsecretarial\\b\", re.IGNORECASE)),\n",
    "        (mr3_re, re.compile(r\"\\bmr\\b|\\bform\\b\", re.IGNORECASE)),\n",
    "        (corp_info_re, re.compile(r\"\\bcorporate\\b\", re.IGNORECASE)),\n",
    "        (auditors_re, re.compile(r\"\\bauditor\\b|\\bindependent\\b\", re.IGNORECASE)),\n",
    "        (corp_gov_re, re.compile(r\"\\bgovernance\\b|\\bcorporate\\b\", re.IGNORECASE)),\n",
    "    ]\n",
    "\n",
    "    # --- 1) Find MD&A title as a structural block (up to 3 joined lines) ---\n",
    "    mdna_start_idx, mdna_end_idx, mdna_block_text = find_title_block_strict(\n",
    "        raw_lines,\n",
    "        mdna_title_re,\n",
    "        max_join_lines=3,\n",
    "        anchor_re=mdna_anchor_re,\n",
    "    )\n",
    "\n",
    "    # --- 1a) Handle special case: MD&A is part of Directors' Report block ---\n",
    "    if mdna_start_idx is None:\n",
    "        logging.info(\"MD&A not found as standalone title; checking inside Directors' Report block\")\n",
    "        directors_anchor_re = re.compile(r\"\\bdirectors\\b\", re.IGNORECASE)\n",
    "        dir_start_idx, dir_end_idx, dir_block_text = find_title_block_strict(\n",
    "            raw_lines,\n",
    "            directors_re,\n",
    "            max_join_lines=3,\n",
    "            anchor_re=directors_anchor_re,\n",
    "        )\n",
    "\n",
    "        if dir_block_text and mdna_title_re.search(dir_block_text):\n",
    "            logging.info(\"MD&A title found inside Directors' Report block; using its boundaries\")\n",
    "            mdna_start_idx = dir_start_idx\n",
    "            mdna_end_idx = dir_end_idx\n",
    "            mdna_block_text = dir_block_text\n",
    "        else:\n",
    "            logging.warning(\"MD&A title block not found in ToC raw lines; skipping\")\n",
    "            return None, None\n",
    "\n",
    "    # --- 2) STRICT page number association: page number after MD&A match, else next 3 lines ---\n",
    "    mdna_page_details = resolve_page_number_for_title_block_strict_with_details(\n",
    "        raw_lines,\n",
    "        title_start_idx=mdna_start_idx,\n",
    "        title_end_idx=mdna_end_idx,\n",
    "        title_re=mdna_title_re,\n",
    "        max_page=max_page,\n",
    "        lookahead_lines=3,\n",
    "    )\n",
    "\n",
    "    # Special case: MD&A is a sub-entry under \"Board's Report including\" with no page number.\n",
    "    # In this layout, the next numeric line belongs to the next sibling (e.g., Annexures),\n",
    "    # so we inherit the parent's page (e.g., 4) and end at the next TRUE top-level section\n",
    "    # (e.g., Corporate Governance at 17 -> end 16).\n",
    "    inherited_parent_for_mdna = False\n",
    "    if mdna_page_details:\n",
    "        page_line_idx = mdna_page_details.get(\"page_line_idx\")\n",
    "        if isinstance(page_line_idx, int) and page_line_idx > (mdna_end_idx + 1):\n",
    "            prev_txt = raw_lines[page_line_idx - 1].get(\"line_text\", \"\")\n",
    "            if re.search(r\"[A-Za-z]\", prev_txt or \"\") and not mdna_title_re.search(prev_txt or \"\"):\n",
    "                parent_page = None\n",
    "                parent_page_line_idx = None\n",
    "                parent_title = None\n",
    "\n",
    "                for back in range(mdna_start_idx - 1, max(-1, mdna_start_idx - 12), -1):\n",
    "                    t = (raw_lines[back].get(\"line_text\", \"\") or \"\").strip()\n",
    "                    if not t:\n",
    "                        continue\n",
    "                    mnum = re.fullmatch(r\"\\s*(\\d{1,3})\\s*\", t)\n",
    "                    if not mnum:\n",
    "                        continue\n",
    "                    n = int(mnum.group(1))\n",
    "                    if not (1 <= n <= max_page):\n",
    "                        continue\n",
    "\n",
    "                    parent_page = n\n",
    "                    parent_page_line_idx = back\n",
    "\n",
    "                    for tt in range(back - 1, max(-1, back - 10), -1):\n",
    "                        cand = (raw_lines[tt].get(\"line_text\", \"\") or \"\").strip()\n",
    "                        if cand and re.search(r\"[A-Za-z]\", cand):\n",
    "                            parent_title = cand\n",
    "                            break\n",
    "\n",
    "                    break\n",
    "\n",
    "                if parent_page is not None and parent_title:\n",
    "                    parent_l = parent_title.lower()\n",
    "                    if (\"including\" in parent_l) and (\"report\" in parent_l) and (\"board\" in parent_l) and (\"directors\" not in parent_l):\n",
    "                        logging.info(\n",
    "                            \"MD&A appears as sub-entry; inheriting parent start page %s from '%s'\",\n",
    "                            parent_page,\n",
    "                            parent_title,\n",
    "                        )\n",
    "                        inherited_parent_for_mdna = True\n",
    "                        mdna_page_details = {\n",
    "                            \"page\": parent_page,\n",
    "                            \"page_span_start\": None,\n",
    "                            \"page_span_end\": None,\n",
    "                            \"page_line_idx\": parent_page_line_idx,\n",
    "                        }\n",
    "\n",
    "    if not mdna_page_details:\n",
    "        logging.warning(\"MD&A start page not found within strict 3-line window; skipping\")\n",
    "        return None, None\n",
    "\n",
    "    start_page = mdna_page_details[\"page\"]\n",
    "\n",
    "    # --- 3) Determine next section page (including same-line multi-column cases) ---\n",
    "    def _next_page_number_in_same_line(line_text: str, after_pos: int, current_start_page: int):\n",
    "        # Prefer numbers that occur after the current entry's page span (when ordering is preserved).\n",
    "        res = _first_valid_page_number_after_pos_with_span(line_text, after_pos, max_page)\n",
    "        if res:\n",
    "            n, _, _ = res\n",
    "            if n > current_start_page:\n",
    "                return n\n",
    "\n",
    "        # Fallback for multi-column merges where extraction order may be scrambled within the same line:\n",
    "        # choose the smallest page number on the line that is greater than the current start page.\n",
    "        candidates = []\n",
    "        for m in _PAGE_INT_RE.finditer(line_text or \"\"):\n",
    "            try:\n",
    "                n = int(m.group(1))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if current_start_page < n <= max_page:\n",
    "                candidates.append(n)\n",
    "\n",
    "        return min(candidates) if candidates else None\n",
    "\n",
    "    def _next_section_start_page_after_line(after_line_idx: int, current_start_page: int):\n",
    "        for j in range(after_line_idx + 1, len(raw_lines)):\n",
    "            txt = raw_lines[j].get(\"line_text\", \"\")\n",
    "            if not re.search(r\"[A-Za-z]\", txt or \"\"):\n",
    "                continue\n",
    "\n",
    "            # Avoid treating the same MD&A title again\n",
    "            if mdna_title_re.search(txt or \"\"):\n",
    "                continue\n",
    "\n",
    "            candidate = resolve_page_number_strict(raw_lines, j, max_page=max_page, lookahead_lines=3)\n",
    "            if candidate is None:\n",
    "                continue\n",
    "\n",
    "            if candidate > current_start_page:\n",
    "                return candidate\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _next_top_level_section_page_after_line(after_line_idx: int, current_start_page: int):\n",
    "        top_level_re = re.compile(\n",
    "            r\"\\b(corporate\\s+governance|auditors?\\s*[’']?\\s*report|independent\\s+auditor|balance\\s+sheet|statement\\s+of\\s+profit|cash\\s+flow\\s+statement|notes)\\b\",\n",
    "            re.IGNORECASE,\n",
    "        )\n",
    "        for j in range(after_line_idx + 1, len(raw_lines)):\n",
    "            txt = raw_lines[j].get(\"line_text\", \"\")\n",
    "            if not re.search(r\"[A-Za-z]\", txt or \"\"):\n",
    "                continue\n",
    "            if not top_level_re.search(txt or \"\"):\n",
    "                continue\n",
    "\n",
    "            candidate = resolve_page_number_strict(raw_lines, j, max_page=max_page, lookahead_lines=3)\n",
    "            if candidate is None:\n",
    "                continue\n",
    "            if candidate > current_start_page:\n",
    "                return candidate\n",
    "\n",
    "        return None\n",
    "\n",
    "    same_line_idx = mdna_page_details[\"page_line_idx\"]\n",
    "    same_line_text = raw_lines[same_line_idx].get(\"line_text\", \"\")\n",
    "    same_line_next_page = _next_page_number_in_same_line(\n",
    "        same_line_text,\n",
    "        after_pos=mdna_page_details.get(\"page_span_end\") or 0,\n",
    "        current_start_page=start_page,\n",
    "    )\n",
    "\n",
    "    next_section_page = same_line_next_page\n",
    "    if next_section_page is None:\n",
    "        if inherited_parent_for_mdna:\n",
    "            next_section_page = _next_top_level_section_page_after_line(mdna_end_idx, start_page)\n",
    "        if next_section_page is None:\n",
    "            next_section_page = _next_section_start_page_after_line(mdna_end_idx, start_page)\n",
    "\n",
    "    end_page = max_page if next_section_page is None else (next_section_page - 1)\n",
    "\n",
    "    if end_page < start_page:\n",
    "        logging.warning(\"Computed invalid MD&A range: start=%s end=%s; skipping\", start_page, end_page)\n",
    "        return None, None\n",
    "\n",
    "    # --- 4) Exclusion ranges (handle same-line next-section; do not require resolving ALL exclusions) ---\n",
    "    def _excluded_ranges():\n",
    "        ranges = []\n",
    "\n",
    "        for title_re, anchor_re in disallowed:\n",
    "            # If MD&A was found inside the Directors' Report, don't treat Directors' Report as an exclusion\n",
    "            if directors_re.pattern == title_re.pattern and mdna_title_re.search(mdna_block_text or \"\"):\n",
    "                if directors_re.search(mdna_block_text or \"\"):\n",
    "                    continue\n",
    "\n",
    "            ex_start_idx, ex_end_idx, ex_block_text = find_title_block_strict(\n",
    "                raw_lines,\n",
    "                title_re,\n",
    "                max_join_lines=3,\n",
    "                anchor_re=anchor_re,\n",
    "            )\n",
    "\n",
    "            if ex_start_idx is None:\n",
    "                continue\n",
    "\n",
    "            ex_details = resolve_page_number_for_title_block_strict_with_details(\n",
    "                raw_lines,\n",
    "                title_start_idx=ex_start_idx,\n",
    "                title_end_idx=ex_end_idx,\n",
    "                title_re=title_re,\n",
    "                max_page=max_page,\n",
    "                lookahead_lines=3,\n",
    "            )\n",
    "\n",
    "            # If exclusion exists but cannot be aligned within strict window, we cannot\n",
    "            # form a reliable range; skip enforcing that specific exclusion.\n",
    "            if not ex_details:\n",
    "                logging.warning(\"Excluded section found but page number not aligned; ignoring exclusion: %s\", ex_block_text)\n",
    "                continue\n",
    "\n",
    "            ex_start_page = ex_details[\"page\"]\n",
    "\n",
    "            ex_same_line_idx = ex_details[\"page_line_idx\"]\n",
    "            ex_same_line_text = raw_lines[ex_same_line_idx].get(\"line_text\", \"\")\n",
    "            ex_same_line_next = _next_page_number_in_same_line(\n",
    "                ex_same_line_text,\n",
    "                after_pos=ex_details.get(\"page_span_end\") or 0,\n",
    "                current_start_page=ex_start_page,\n",
    "            )\n",
    "\n",
    "            ex_next_page = ex_same_line_next\n",
    "            if ex_next_page is None:\n",
    "                ex_next_page = _next_section_start_page_after_line(ex_end_idx, ex_start_page)\n",
    "\n",
    "            # Critical safety for multi-column/boxed ToCs:\n",
    "            # if an excluded section starts before MD&A (by page number), it must end no later\n",
    "            # than the MD&A start page (as both are ToC-derived section starts), even if the\n",
    "            # extracted line order is scrambled.\n",
    "            if ex_start_page < start_page:\n",
    "                if ex_next_page is None or start_page < ex_next_page:\n",
    "                    ex_next_page = start_page\n",
    "\n",
    "            ex_end_page = max_page if ex_next_page is None else (ex_next_page - 1)\n",
    "\n",
    "            ranges.append(\n",
    "                {\n",
    "                    \"title\": ex_block_text,\n",
    "                    \"start\": ex_start_page,\n",
    "                    \"end\": ex_end_page,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return ranges\n",
    "\n",
    "    ex_ranges = _excluded_ranges()\n",
    "\n",
    "    for r in ex_ranges:\n",
    "        if r[\"start\"] <= start_page <= r[\"end\"]:\n",
    "            logging.warning(\n",
    "                \"MD&A start page %s falls inside excluded section '%s' (%s-%s); skipping\",\n",
    "                start_page,\n",
    "                r[\"title\"],\n",
    "                r[\"start\"],\n",
    "                r[\"end\"],\n",
    "            )\n",
    "            return None, None\n",
    "\n",
    "    logging.info(\"MD&A boundaries (STRICT ToC blocks): start=%s, end=%s\", start_page, end_page)\n",
    "    return start_page, end_page\n",
    "\n",
    "\n",
    "def detect_mdna_boundaries(pages_text, toc_start_page=None, company_folder: str | None = None):\n",
    "    \"\"\"Detect MD&A boundaries using STRICT Table of Contents (ToC) rules ONLY.\n",
    "\n",
    "    Required behavior:\n",
    "      - ToC is the only source of truth (first 5 pages only).\n",
    "      - Treat ToC as STRUCTURAL BLOCKS: titles may be wrapped across lines.\n",
    "      - When MD&A title is detected, search ONLY next 3 extracted lines for page number.\n",
    "      - Deterministic alignment, no body-text inference; if not resolvable, SKIP.\n",
    "\n",
    "    Amit_spinning behavior (hybrid, still ToC-only):\n",
    "      - First try the normal strict-ToC detector (works for Amit PDFs that look like other companies).\n",
    "      - If it fails to resolve boundaries, fall back to INDEX-style detection for Amit PDFs whose\n",
    "        MD&A is embedded under \"Board’s Report Including ...\".\n",
    "\n",
    "    Returns:\n",
    "      (start_page, end_page) or (None, None)\n",
    "    \"\"\"\n",
    "\n",
    "    max_page = len(pages_text)\n",
    "    if max_page <= 0:\n",
    "        logging.warning(\"Empty document; cannot detect MD&A boundaries\")\n",
    "        return None, None\n",
    "\n",
    "    raw_lines = collect_toc_raw_lines(pages_text, max_pages=5)\n",
    "    if not raw_lines:\n",
    "        logging.warning(\"No ToC raw lines detected in the first 5 pages\")\n",
    "        return None, None\n",
    "\n",
    "    # 1) Always try the normal strict-ToC path first.\n",
    "    start_page, end_page = _detect_mdna_boundaries_strict_toc(raw_lines, max_page=max_page)\n",
    "    if start_page is not None and end_page is not None:\n",
    "        return start_page, end_page\n",
    "\n",
    "    # 2) Amit_spinning fallback: INDEX-style.\n",
    "    if company_folder == \"Amit_spinning\":\n",
    "        return _detect_mdna_boundaries_amit_spinning_index(raw_lines, max_page=max_page)\n",
    "\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c7f4c",
   "metadata": {},
   "source": [
    "### 7. MD&A Text Extraction (Boundary-Aware) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34d448a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mdna_text(pages_text, start_page, end_page):\n",
    "    \"\"\"Extract raw MD&A text from PDF pages using detected boundaries.\n",
    "\n",
    "    Args:\n",
    "        pages_text: List of page dictionaries from PDFInterface.get_pages_text(),\n",
    "                    each like {\"page_number\": int, \"text\": str}\n",
    "        start_page: 1-based start page number (inclusive)\n",
    "        end_page: 1-based end page number (inclusive)\n",
    "\n",
    "    Returns:\n",
    "        str: Concatenated raw MD&A text (no cleaning), with double newlines\n",
    "             inserted between pages.\n",
    "    \"\"\"\n",
    "\n",
    "    included_text_chunks = []\n",
    "    included_pages = 0\n",
    "\n",
    "    for page in pages_text:\n",
    "        page_no = page.get(\"page_number\")\n",
    "        if page_no is None:\n",
    "            continue\n",
    "\n",
    "        if start_page <= page_no <= end_page:\n",
    "            included_pages += 1\n",
    "            included_text_chunks.append(page.get(\"text\", \"\"))\n",
    "\n",
    "    mdna_text = \"\\n\\n\".join(included_text_chunks)\n",
    "\n",
    "    logging.info(\"MD&A pages included: %d\", included_pages)\n",
    "    logging.info(\"Extracted MD&A text length (chars): %d\", len(mdna_text))\n",
    "\n",
    "    return mdna_text\n",
    "\n",
    "\n",
    "# mdna_text = extract_mdna_text(pages, start_page, end_page)\n",
    "\n",
    "# print(\"MD&A preview:\\n\")\n",
    "# print(mdna_text[:1000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b0200",
   "metadata": {},
   "source": [
    "### 8. MD&A Text Cleaning & Artifact Removal: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08b64a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mdna_text(raw_text):\n",
    "    \"\"\"Conservatively clean extracted MD&A text.\n",
    "\n",
    "    What this does (conservative heuristics):\n",
    "    - Removes obvious repeating headers/footers (e.g., 'Annual Report ...', standalone page numbers,\n",
    "      and very header-like company-name lines when they appear repeatedly).\n",
    "    - Removes obvious table artifacts (high digit-density lines, and separator-only lines).\n",
    "    - Normalizes whitespace while preserving paragraph breaks.\n",
    "\n",
    "    What this does NOT do:\n",
    "    - Does not lowercase text\n",
    "    - Does not remove punctuation\n",
    "    - Does not change wording\n",
    "\n",
    "    Args:\n",
    "        raw_text: str\n",
    "\n",
    "    Returns:\n",
    "        str: cleaned MD&A text\n",
    "    \"\"\"\n",
    "\n",
    "    if raw_text is None:\n",
    "        raw_text = \"\"\n",
    "\n",
    "    original_len = len(raw_text)\n",
    "\n",
    "    # Split into lines to enable conservative line-based removals.\n",
    "    lines = raw_text.splitlines()\n",
    "\n",
    "    # Pre-compute line frequencies (normalized) to detect repeated headers/footers.\n",
    "    def _norm_line_for_freq(line: str) -> str:\n",
    "        return re.sub(r\"\\s+\", \" \", (line or \"\").strip())\n",
    "\n",
    "    normalized_lines = [_norm_line_for_freq(ln) for ln in lines]\n",
    "    freq = {}\n",
    "    for nl in normalized_lines:\n",
    "        if not nl:\n",
    "            continue\n",
    "        freq[nl] = freq.get(nl, 0) + 1\n",
    "\n",
    "    annual_report_re = re.compile(r\"^\\s*Annual\\s+Report(?:\\s+\\d{4}\\s*[-–]\\s*\\d{2,4})?\\s*$\", re.IGNORECASE)\n",
    "    standalone_page_no_re = re.compile(r\"^\\s*(?:page\\s*)?\\d{1,4}\\s*$\", re.IGNORECASE)\n",
    "    separators_only_re = re.compile(r\"^[\\s\\-_=*~•·\\.\\|:;,+/\\\\]+$\")\n",
    "\n",
    "    def _is_company_name_like(line: str) -> bool:\n",
    "        # Conservative: only remove if it looks like a standalone header/footer line.\n",
    "        s = (line or \"\").strip()\n",
    "        if not s:\n",
    "            return False\n",
    "        if len(s) > 90:\n",
    "            return False\n",
    "\n",
    "        # Must contain common company suffixes.\n",
    "        if not re.search(r\"\\b(LIMITED|LTD\\.?|PVT\\.?\\s+LTD\\.?|PRIVATE\\s+LIMITED)\\b\", s, flags=re.IGNORECASE):\n",
    "            return False\n",
    "\n",
    "        # Must be mostly uppercase (typical header styling).\n",
    "        letters = re.findall(r\"[A-Za-z]\", s)\n",
    "        if not letters:\n",
    "            return False\n",
    "        upper_letters = sum(1 for ch in letters if ch.isupper())\n",
    "        if upper_letters / len(letters) < 0.8:\n",
    "            return False\n",
    "\n",
    "        # Keep it short in words (header/footer line).\n",
    "        if len(s.split()) > 10:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _is_high_numeric_density(line: str) -> bool:\n",
    "        s = (line or \"\").strip()\n",
    "        if len(s) < 12:\n",
    "            return False\n",
    "        # Density computed over non-space characters.\n",
    "        compact = re.sub(r\"\\s+\", \"\", s)\n",
    "        if not compact:\n",
    "            return False\n",
    "        digits = sum(1 for ch in compact if ch.isdigit())\n",
    "        return (digits / len(compact)) > 0.40\n",
    "\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for raw_line, norm_line in zip(lines, normalized_lines):\n",
    "        s = (raw_line or \"\").rstrip()\n",
    "        sn = norm_line\n",
    "\n",
    "        # Remove obvious separators/formatting-only lines.\n",
    "        if sn and separators_only_re.match(sn):\n",
    "            continue\n",
    "\n",
    "        # Remove obvious page numbers (standalone).\n",
    "        if sn and standalone_page_no_re.match(sn):\n",
    "            continue\n",
    "\n",
    "        # Remove 'Annual Report' headers/footers.\n",
    "        if sn and annual_report_re.match(sn):\n",
    "            continue\n",
    "\n",
    "        # Remove frequent header/footer-like lines conservatively.\n",
    "        # (Only if repeated AND header-ish AND not too long.)\n",
    "        if sn and freq.get(sn, 0) >= 3:\n",
    "            if _is_company_name_like(sn) or annual_report_re.match(sn) or standalone_page_no_re.match(sn):\n",
    "                continue\n",
    "\n",
    "        # Remove obvious table artifacts: high numeric density.\n",
    "        if _is_high_numeric_density(sn):\n",
    "            continue\n",
    "\n",
    "        # Whitespace normalization inside the line.\n",
    "        s = re.sub(r\"[ \\t]{2,}\", \" \", s).strip(\" \")\n",
    "        cleaned_lines.append(s)\n",
    "\n",
    "    # Re-join with newlines and normalize paragraph spacing.\n",
    "    cleaned_text = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "    # Reduce 3+ consecutive newlines to at most 2.\n",
    "    cleaned_text = re.sub(r\"\\n{3,}\", \"\\n\\n\", cleaned_text)\n",
    "\n",
    "    # Trim leading/trailing whitespace/newlines.\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    logging.info(\"MD&A text original length (chars): %d\", original_len)\n",
    "    logging.info(\"MD&A text cleaned length (chars): %d\", len(cleaned_text))\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "#sanity test: \n",
    "# cleaned_text = clean_mdna_text(mdna_text)\n",
    "\n",
    "# print(\"Cleaned MD&A preview:\\n\")\n",
    "# print(cleaned_text[:1200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d36041d",
   "metadata": {},
   "source": [
    "### 9. MD&A Quality Verification & Validation Metrics : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5792c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_mdna_quality(mdna_text, pages_match_toc: bool):\n",
    "    \"\"\"Verify extracted/cleaned MD&A text quality using STRICT semantics.\n",
    "\n",
    "    quality_passed MUST be TRUE only if:\n",
    "      - Extracted pages match the ToC-declared MD&A range exactly (pages_match_toc=True)\n",
    "      - Text contains at least 2 MD&A-specific phrases (case-insensitive)\n",
    "\n",
    "    Guardrail:\n",
    "      - Audit / Corporate / Director section signals must ALWAYS fail quality.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys:\n",
    "          - word_count (int)\n",
    "          - narrative_density (float)\n",
    "          - keyword_hits (list[str])\n",
    "          - quality_passed (bool)\n",
    "    \"\"\"\n",
    "\n",
    "    if mdna_text is None:\n",
    "        mdna_text = \"\"\n",
    "\n",
    "    # a) Word count\n",
    "    words = re.findall(r\"\\b\\w+\\b\", mdna_text)\n",
    "    word_count = len(words)\n",
    "\n",
    "    # b) Narrative density (alphabetic chars / total chars)\n",
    "    total_chars = len(mdna_text)\n",
    "    alpha_chars = sum(1 for ch in mdna_text if ch.isalpha())\n",
    "    narrative_density = (alpha_chars / total_chars) if total_chars else 0.0\n",
    "\n",
    "    lower_text = mdna_text.lower()\n",
    "\n",
    "    # c) Required MD&A phrases\n",
    "    mdna_phrases = [\n",
    "        \"industry outlook\",\n",
    "        \"opportunities and threats\",\n",
    "        \"risk management\",\n",
    "        \"future outlook\",\n",
    "        \"segment performance\",\n",
    "        \"global economy\",\n",
    "    ]\n",
    "\n",
    "    keyword_hits = [p for p in mdna_phrases if p in lower_text]\n",
    "\n",
    "    # Disqualifiers: MUST always fail\n",
    "    disqualifiers = [\n",
    "        \"independent auditor\",\n",
    "        \"auditor's report\",\n",
    "        \"auditors' report\",\n",
    "        \"auditors report\",\n",
    "        \"secretarial audit\",\n",
    "        \"form mr-3\",\n",
    "        \"mr-3\",\n",
    "        \"corporate information\",\n",
    "        \"corporate governance\",\n",
    "        \"directors' report\",\n",
    "        \"director's report\",\n",
    "        \"directors report\",\n",
    "    ]\n",
    "\n",
    "    disqualifier_hit = any(bad in lower_text for bad in disqualifiers)\n",
    "\n",
    "    criteria_pages_match = bool(pages_match_toc)\n",
    "    criteria_phrases = len(keyword_hits) >= 2\n",
    "\n",
    "    quality_passed = bool(criteria_pages_match and criteria_phrases and (not disqualifier_hit))\n",
    "\n",
    "    logging.info(\"MD&A quality — word_count: %d\", word_count)\n",
    "    logging.info(\"MD&A quality — narrative_density: %.4f\", narrative_density)\n",
    "    logging.info(\"MD&A quality — keyword_hits (%d): %s\", len(keyword_hits), keyword_hits)\n",
    "    logging.info(\"MD&A quality — pages_match_toc: %s\", criteria_pages_match)\n",
    "    logging.info(\"MD&A quality — disqualifier_hit: %s\", disqualifier_hit)\n",
    "\n",
    "    if quality_passed:\n",
    "        logging.info(\"MD&A quality PASSED\")\n",
    "    else:\n",
    "        logging.warning(\n",
    "            \"MD&A quality FLAGGED (pages_match_toc=%s, phrases_ok=%s, disqualifier_hit=%s)\",\n",
    "            criteria_pages_match,\n",
    "            criteria_phrases,\n",
    "            disqualifier_hit,\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"word_count\": word_count,\n",
    "        \"narrative_density\": float(narrative_density),\n",
    "        \"keyword_hits\": keyword_hits,\n",
    "        \"quality_passed\": quality_passed,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983ca5a",
   "metadata": {},
   "source": [
    "### 10. Page Offset Solver & Validation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d77b573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def _norm_for_match(s: str) -> str:\n",
    "    s = (s or \"\").lower()\n",
    "    # normalize common apostrophes and whitespace\n",
    "    s = s.replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def _extract_printed_page_number_from_page_text(text: str) -> int | None:\n",
    "    \"\"\"Best-effort parse of the *printed* page number from footer/header.\n",
    "\n",
    "    Heuristic: look at last ~15 non-empty lines and pick a small, standalone integer\n",
    "    (e.g., \"23\", \"- 23 -\", \"Page 23\").\n",
    "    \"\"\"\n",
    "\n",
    "    lines = [ln.strip() for ln in (text or \"\").splitlines() if (ln or \"\").strip()]\n",
    "    if not lines:\n",
    "        return None\n",
    "\n",
    "    tail = lines[-15:]\n",
    "\n",
    "    patterns = [\n",
    "        re.compile(r\"^page\\s*(\\d{1,4})\\s*$\", re.IGNORECASE),\n",
    "        re.compile(r\"^[-–—]*\\s*(\\d{1,4})\\s*[-–—]*$\"),\n",
    "        re.compile(r\"^\\(?\\s*(\\d{1,4})\\s*\\)?$\"),\n",
    "    ]\n",
    "\n",
    "    for ln in reversed(tail):\n",
    "        # avoid matching years or long numeric strings\n",
    "        if re.search(r\"\\b(19|20)\\d{2}\\b\", ln):\n",
    "            continue\n",
    "        if len(ln) > 20:\n",
    "            continue\n",
    "\n",
    "        for pat in patterns:\n",
    "            m = pat.match(ln)\n",
    "            if not m:\n",
    "                continue\n",
    "            try:\n",
    "                n = int(m.group(1))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if 1 <= n <= 5000:\n",
    "                return n\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def build_printed_to_pdf_page_map(pages_text: list[dict]) -> dict[int, int]:\n",
    "    \"\"\"Map printed page number -> PDF page index (1-based).\"\"\"\n",
    "\n",
    "    out: dict[int, int] = {}\n",
    "    for p in pages_text:\n",
    "        pdf_page = p.get(\"page_number\")\n",
    "        if not isinstance(pdf_page, int):\n",
    "            continue\n",
    "        printed = _extract_printed_page_number_from_page_text(p.get(\"text\", \"\") or \"\")\n",
    "        if printed is None:\n",
    "            continue\n",
    "        # Keep the first occurrence (most docs have a 1-1 mapping)\n",
    "        out.setdefault(int(printed), int(pdf_page))\n",
    "    return out\n",
    "\n",
    "\n",
    "class PageOffsetSolver:\n",
    "    \"\"\"Compute delta between printed ToC page numbers and PDF page indices.\n",
    "\n",
    "    Anchor-based approach:\n",
    "      1) Find anchor ('Independent Auditor's Report') in ToC lines and read its ToC page number.\n",
    "      2) Find the same anchor in the actual PDF pages (header/title text) to get PDF page index.\n",
    "      3) delta = pdf_page_index - toc_page_number\n",
    "\n",
    "    Returns 0 if anchor can't be resolved safely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, doc: fitz.Document, pages_text: list[dict], toc_max_pages: int = 15):\n",
    "        self.doc = doc\n",
    "        self.pages_text = pages_text\n",
    "        self.toc_raw_lines = collect_toc_raw_lines(pages_text, max_pages=toc_max_pages)\n",
    "\n",
    "    def find_offset(self) -> int:\n",
    "        toc_anchor_page = self._find_anchor_page_in_toc()\n",
    "        if toc_anchor_page is None:\n",
    "            return 0\n",
    "\n",
    "        pdf_anchor_page = self._find_anchor_page_in_pdf()\n",
    "        if pdf_anchor_page is None:\n",
    "            return 0\n",
    "\n",
    "        return int(pdf_anchor_page - toc_anchor_page)\n",
    "\n",
    "    def _find_anchor_page_in_toc(self) -> int | None:\n",
    "        # Match variants like: Independent Auditor’s Report / Independent Auditors' Report\n",
    "        anchor_re = re.compile(r\"\\bindependent\\s+auditors?\\s*[’']?\\s*report\\b\", re.IGNORECASE)\n",
    "        max_page = len(self.pages_text)\n",
    "\n",
    "        for i, item in enumerate(self.toc_raw_lines):\n",
    "            line = item.get(\"line_text\", \"\") or \"\"\n",
    "            if not anchor_re.search(line):\n",
    "                continue\n",
    "\n",
    "            page = resolve_page_number_strict(self.toc_raw_lines, i, max_page=max_page, lookahead_lines=3)\n",
    "            if isinstance(page, int) and page >= 1:\n",
    "                return page\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _find_anchor_page_in_pdf(self, search_limit: int = 250) -> int | None:\n",
    "        anchor_re = re.compile(r\"\\bindependent\\s+auditors?\\s*[’']?\\s*report\\b\", re.IGNORECASE)\n",
    "        limit = min(search_limit, self.doc.page_count)\n",
    "\n",
    "        for page_idx0 in range(limit):\n",
    "            page = self.doc.load_page(page_idx0)\n",
    "            text = page.get_text(\"text\") or \"\"\n",
    "\n",
    "            # Prefer header-ish region: first ~30 non-empty lines\n",
    "            lines = [ln.strip() for ln in text.splitlines() if (ln or \"\").strip()]\n",
    "            header_text = \"\\n\".join(lines[:30])\n",
    "\n",
    "            if anchor_re.search(header_text) or anchor_re.search(text):\n",
    "                return page_idx0 + 1\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "def validate_and_adjust_start_page(doc: fitz.Document, candidate_start_page_1based: int) -> int:\n",
    "    \"\"\"Validate MD&A start page using keyword check; if miss, search +/- 3 pages.\"\"\"\n",
    "\n",
    "    keywords_re = re.compile(r\"management\\s+discussion|\\bstructure\\b|\\boutlook\\b\", re.IGNORECASE)\n",
    "\n",
    "    def _page_has_keywords(page_1based: int) -> bool:\n",
    "        if not (1 <= page_1based <= doc.page_count):\n",
    "            return False\n",
    "        text = doc.load_page(page_1based - 1).get_text(\"text\") or \"\"\n",
    "        return bool(keywords_re.search(text))\n",
    "\n",
    "    if _page_has_keywords(candidate_start_page_1based):\n",
    "        return candidate_start_page_1based\n",
    "\n",
    "    for delta in range(1, 4):\n",
    "        for p in (candidate_start_page_1based - delta, candidate_start_page_1based + delta):\n",
    "            if _page_has_keywords(p):\n",
    "                return p\n",
    "\n",
    "    return candidate_start_page_1based\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dad837",
   "metadata": {},
   "source": [
    "### 11. End-to-End MD&A Extraction Pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "797991f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visual Anchor Search core (with quality fixes) ---\n",
    "\n",
    "try:\n",
    "    from fuzzywuzzy import fuzz  # type: ignore\n",
    "except Exception:\n",
    "    fuzz = None\n",
    "\n",
    "\n",
    "def _norm_text_for_fuzzy(s: str) -> str:\n",
    "    s = (s or \"\").lower()\n",
    "    s = s.replace(\"&\", \"and\")\n",
    "    s = s.replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def _strip_title_noise(s: str) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    s = re.sub(r\"\\b\\d{1,4}\\b\", \" \", s)\n",
    "    s = re.sub(r\"[.·•]{2,}\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip(\" -:\\t\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def _count_words(text: str) -> int:\n",
    "    return len(re.findall(r\"\\b\\w+\\b\", text or \"\"))\n",
    "\n",
    "\n",
    "def _phrase_to_line_start_regex(phrase: str) -> str:\n",
    "    s = (phrase or \"\").strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    parts = [re.escape(p) for p in re.split(r\"\\s+\", s) if p]\n",
    "    if not parts:\n",
    "        return \"\"\n",
    "    return r\"[\\s\\u00a0]+\".join(parts) + r\"\\b\"\n",
    "\n",
    "\n",
    "def _phrase_to_anywhere_regex(phrase: str) -> str:\n",
    "    s = (phrase or \"\").strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    parts = [re.escape(p) for p in re.split(r\"\\s+\", s) if p]\n",
    "    if not parts:\n",
    "        return \"\"\n",
    "    return r\"\\b\" + r\"[\\s\\u00a0]+\".join(parts) + r\"\\b\"\n",
    "\n",
    "\n",
    "def _has_disqualifier_near_top(text: str) -> bool:\n",
    "    head = (text or \"\")[:2500]\n",
    "    disqualifiers = [\n",
    "        r\"DIRECTORS\\s*[’']?\\s*REPORT\",\n",
    "        r\"REPORT\\s+OF\\s+THE\\s+DIRECTORS\",\n",
    "        r\"BOARD\\s*[’']?\\s*REPORT\",\n",
    "        r\"NOTICE\\b\",\n",
    "        r\"CORPORATE\\s+INFORMATION\",\n",
    "        r\"CHAIRMAN\\b\",\n",
    "        r\"BOARD\\s+OF\\s+DIRECTORS\",\n",
    "        r\"COMPANY\\s+SECRETARY\",\n",
    "        r\"SECRETARY\\b\",\n",
    "        r\"AGM\\b\",\n",
    "        r\"SPECIAL\\s+RESOLUTIONS\\b\",\n",
    "        r\"SHAREHOLDING\\s+PATTERN\\b\",\n",
    "        r\"CORPORATE\\s+GOVERNANCE\\b\",\n",
    "    ]\n",
    "    for pat in disqualifiers:\n",
    "        if re.search(pat, head, flags=re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def find_real_page_index(\n",
    "    doc: fitz.Document,\n",
    "    section_title: str,\n",
    "    toc_page_num: int,\n",
    "    window: int = 5,\n",
    "    header_ratio: float = 0.20,\n",
    "    min_score: int = 85,\n",
    ") -> int | None:\n",
    "    if not section_title or not isinstance(toc_page_num, int):\n",
    "        return None\n",
    "\n",
    "    if doc.page_count <= 0:\n",
    "        return None\n",
    "\n",
    "    title_norm = _norm_text_for_fuzzy(section_title)\n",
    "    if not title_norm:\n",
    "        return None\n",
    "\n",
    "    hint0 = toc_page_num - 1\n",
    "    start0 = max(0, hint0 - window)\n",
    "    end0 = min(doc.page_count - 1, hint0 + window)\n",
    "\n",
    "    best_page_1based: int | None = None\n",
    "    best_score = -1\n",
    "\n",
    "    for page_idx0 in range(start0, end0 + 1):\n",
    "        page = doc.load_page(page_idx0)\n",
    "        rect = page.rect\n",
    "\n",
    "        header_h = rect.height * float(header_ratio)\n",
    "        clip = fitz.Rect(rect.x0, rect.y0, rect.x1, rect.y0 + header_h)\n",
    "\n",
    "        header_text = page.get_text(\"text\", clip=clip) or \"\"\n",
    "        header_norm = _norm_text_for_fuzzy(header_text)\n",
    "        if not header_norm:\n",
    "            continue\n",
    "\n",
    "        if fuzz is None:\n",
    "            score = 100 if title_norm in header_norm else 0\n",
    "        else:\n",
    "            score = int(fuzz.token_set_ratio(title_norm, header_norm))\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_page_1based = page_idx0 + 1\n",
    "\n",
    "    if best_page_1based is not None and best_score >= int(min_score):\n",
    "        return best_page_1based\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _find_candidate_pages_by_header_match(\n",
    "    doc: fitz.Document,\n",
    "    section_title: str,\n",
    "    toc_page_num: int,\n",
    "    window: int = 5,\n",
    "    header_ratio: float = 0.20,\n",
    "    min_score: int = 85,\n",
    ") -> list[tuple[int, int]]:\n",
    "    if not section_title or not isinstance(toc_page_num, int) or doc.page_count <= 0:\n",
    "        return []\n",
    "\n",
    "    title_norm = _norm_text_for_fuzzy(section_title)\n",
    "    if not title_norm:\n",
    "        return []\n",
    "\n",
    "    hint0 = toc_page_num - 1\n",
    "    start0 = max(0, hint0 - window)\n",
    "    end0 = min(doc.page_count - 1, hint0 + window)\n",
    "\n",
    "    out: list[tuple[int, int]] = []\n",
    "\n",
    "    for page_idx0 in range(start0, end0 + 1):\n",
    "        page = doc.load_page(page_idx0)\n",
    "        rect = page.rect\n",
    "        header_h = rect.height * float(header_ratio)\n",
    "        clip = fitz.Rect(rect.x0, rect.y0, rect.x1, rect.y0 + header_h)\n",
    "\n",
    "        header_text = page.get_text(\"text\", clip=clip) or \"\"\n",
    "        header_norm = _norm_text_for_fuzzy(header_text)\n",
    "        if not header_norm:\n",
    "            continue\n",
    "\n",
    "        if fuzz is None:\n",
    "            score = 100 if title_norm in header_norm else 0\n",
    "        else:\n",
    "            score = int(fuzz.token_set_ratio(title_norm, header_norm))\n",
    "\n",
    "        if score >= int(min_score):\n",
    "            out.append((page_idx0 + 1, score))\n",
    "\n",
    "    out.sort(key=lambda x: x[0])\n",
    "    return out\n",
    "\n",
    "\n",
    "def _find_candidate_pages_by_top_lines(\n",
    "    pages_text: list[dict],\n",
    "    start_page: int,\n",
    "    max_forward: int,\n",
    "    pattern: re.Pattern,\n",
    "    max_lines: int = 150,\n",
    ") -> list[int]:\n",
    "    max_page = len(pages_text)\n",
    "    s = max(1, int(start_page))\n",
    "    e = min(max_page, int(start_page) + int(max_forward))\n",
    "\n",
    "    out: list[int] = []\n",
    "    for p in range(s, e + 1):\n",
    "        t = (pages_text[p - 1].get(\"text\") or \"\")\n",
    "        if not t:\n",
    "            continue\n",
    "        top = \"\\n\".join((t.splitlines() or [])[: int(max_lines)])\n",
    "        if pattern.search(top):\n",
    "            if _has_disqualifier_near_top(top):\n",
    "                continue\n",
    "            out.append(p)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _find_next_section_hint_from_toc(pages_text: list[dict], after_page: int, max_pages: int = 5):\n",
    "    raw_lines = collect_toc_raw_lines(pages_text, max_pages=max_pages)\n",
    "    if not raw_lines:\n",
    "        return None, None\n",
    "\n",
    "    max_page = len(pages_text)\n",
    "\n",
    "    best = None\n",
    "    for i in range(len(raw_lines)):\n",
    "        line_text = raw_lines[i].get(\"line_text\", \"\") or \"\"\n",
    "        if not re.search(r\"[A-Za-z]\", line_text):\n",
    "            continue\n",
    "\n",
    "        page = resolve_page_number_strict(raw_lines, i, max_page=max_page, lookahead_lines=3)\n",
    "        if page is None or page <= after_page:\n",
    "            continue\n",
    "\n",
    "        title = _strip_title_noise(line_text)\n",
    "        if not title:\n",
    "            continue\n",
    "\n",
    "        if best is None or page < best[0]:\n",
    "            best = (page, title)\n",
    "\n",
    "    if best is None:\n",
    "        return None, None\n",
    "\n",
    "    return best[1], best[0]\n",
    "\n",
    "\n",
    "def _find_section_title_for_page_from_toc(\n",
    "    pages_text: list[dict],\n",
    "    target_page: int,\n",
    "    max_pages: int = 5,\n",
    ") -> str | None:\n",
    "    \"\"\"Return the ToC section title whose resolved page number == target_page.\"\"\"\n",
    "\n",
    "    raw_lines = collect_toc_raw_lines(pages_text, max_pages=max_pages)\n",
    "    if not raw_lines:\n",
    "        return None\n",
    "\n",
    "    max_page = len(pages_text)\n",
    "\n",
    "    best_title = None\n",
    "    best_len = -1\n",
    "\n",
    "    for i in range(len(raw_lines)):\n",
    "        line_text = raw_lines[i].get(\"line_text\", \"\") or \"\"\n",
    "        if not re.search(r\"[A-Za-z]\", line_text):\n",
    "            continue\n",
    "\n",
    "        page = resolve_page_number_strict(raw_lines, i, max_page=max_page, lookahead_lines=3)\n",
    "        if page is None or int(page) != int(target_page):\n",
    "            continue\n",
    "\n",
    "        title = _strip_title_noise(line_text)\n",
    "        if not title:\n",
    "            continue\n",
    "\n",
    "        # Avoid generic ToC noise.\n",
    "        if re.search(r\"\\bcontents\\b\", title, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        if len(title) > best_len:\n",
    "            best_title = title\n",
    "            best_len = len(title)\n",
    "\n",
    "    return best_title\n",
    "\n",
    "\n",
    "def extract_text_range(\n",
    "    pages_text: list[dict],\n",
    "    start_page: int,\n",
    "    end_page: int,\n",
    "    stop_phrases: list[str] | None = None,\n",
    ") -> str:\n",
    "    \"\"\"Extract text from [start_page, end_page] with tail chopping on the last content page.\"\"\"\n",
    "\n",
    "    if start_page is None or end_page is None:\n",
    "        return \"\"\n",
    "\n",
    "    stop_phrases = [p for p in (stop_phrases or []) if p]\n",
    "\n",
    "    stop_regex_line = None\n",
    "    stop_regex_any = None\n",
    "\n",
    "    if stop_phrases:\n",
    "        alts_line = [r for r in (_phrase_to_line_start_regex(p) for p in stop_phrases) if r]\n",
    "        if alts_line:\n",
    "            stop_regex_line = re.compile(\n",
    "                r\"(?im)^(?:[\\s\\u00a0]*\\d{1,4}[\\s\\u00a0]+)?[\\s\\u00a0]*(?:\" + \"|\".join(alts_line) + r\")\"\n",
    "            )\n",
    "\n",
    "        alts_any = [r for r in (_phrase_to_anywhere_regex(p) for p in stop_phrases) if r]\n",
    "        if alts_any:\n",
    "            stop_regex_any = re.compile(r\"(?i)(?:\" + \"|\".join(alts_any) + r\")\")\n",
    "\n",
    "    def _first_match_after(rx: re.Pattern, s: str, min_index: int) -> int | None:\n",
    "        for m in rx.finditer(s):\n",
    "            if m.start() >= int(min_index):\n",
    "                return m.start()\n",
    "        return None\n",
    "\n",
    "    selected: list[tuple[int, str]] = []\n",
    "    for page in pages_text:\n",
    "        page_no = page.get(\"page_number\")\n",
    "        if not isinstance(page_no, int):\n",
    "            continue\n",
    "        if page_no < start_page or page_no > end_page:\n",
    "            continue\n",
    "        selected.append((page_no, page.get(\"text\", \"\") or \"\"))\n",
    "\n",
    "    if not selected:\n",
    "        return \"\"\n",
    "\n",
    "    # Iteratively tail-chop: if chopping removes the entire last page, re-apply to the new last page.\n",
    "    for _ in range(2):\n",
    "        last_idx = None\n",
    "        for i in range(len(selected) - 1, -1, -1):\n",
    "            if (selected[i][1] or \"\").strip():\n",
    "                last_idx = i\n",
    "                break\n",
    "\n",
    "        if last_idx is None:\n",
    "            break\n",
    "\n",
    "        if stop_regex_line is None and stop_regex_any is None:\n",
    "            break\n",
    "\n",
    "        page_no, text = selected[last_idx]\n",
    "        cut_at = None\n",
    "\n",
    "        text_len = len(text or \"\")\n",
    "        min_any = 0 if text_len < 1200 else 400\n",
    "\n",
    "        if stop_regex_line is not None:\n",
    "            cut_at = _first_match_after(stop_regex_line, text, min_index=0)\n",
    "\n",
    "        if cut_at is None and stop_regex_any is not None:\n",
    "            cut_at = _first_match_after(stop_regex_any, text, min_index=min_any)\n",
    "\n",
    "        if cut_at is None:\n",
    "            break\n",
    "\n",
    "        new_text = text[:cut_at]\n",
    "        selected[last_idx] = (page_no, new_text)\n",
    "\n",
    "        # If we didn't wipe the page, we're done.\n",
    "        if (new_text or \"\").strip():\n",
    "            break\n",
    "\n",
    "    return \"\\n\\n\".join(t for _p, t in selected)\n",
    "\n",
    "\n",
    "def _clean_mdna_headers_only(text: str, company_name: str | None) -> str:\n",
    "    company_norm = _norm_text_for_fuzzy(company_name or \"\")\n",
    "\n",
    "    out_lines = []\n",
    "    for ln in (text or \"\").splitlines():\n",
    "        s = (ln or \"\").strip()\n",
    "        if not s:\n",
    "            out_lines.append(\"\")\n",
    "            continue\n",
    "\n",
    "        sn = _norm_text_for_fuzzy(s)\n",
    "\n",
    "        if company_norm and sn == company_norm:\n",
    "            continue\n",
    "        if re.search(r\"\\bannual\\s+report\\b\", s, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        out_lines.append(s)\n",
    "\n",
    "    cleaned = \"\\n\".join(out_lines)\n",
    "    cleaned = re.sub(r\"\\n{3,}\", \"\\n\\n\", cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def _trim_before_mdna_heading_for_amit(text: str) -> str:\n",
    "    \"\"\"Amit_spinning only: trim Directors/Board report lead-in before MD&A-like subheading.\"\"\"\n",
    "\n",
    "    rx = re.compile(\n",
    "        r\"(?im)^\\s*(?:\\(?\\d+\\)?\\s*[\\).:-]\\s*)?(?:\"\n",
    "        r\"FINANCIAL\\s+ANALYSIS\\s+AND\\s+PERFORMANCE\\s+REVIEW\"\n",
    "        r\"|MANAGEMENT\\s+DISCUSSION(?:\\s+(?:AND|&)\\s+ANALYSIS)?\"\n",
    "        r\"|MD\\s*&\\s*A\"\n",
    "        r\")\\b\"\n",
    "    )\n",
    "    m = rx.search(text or \"\")\n",
    "    if m and m.start() > 0:\n",
    "        return (text[m.start() :]).lstrip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_mdna_block(\n",
    "    doc: fitz.Document,\n",
    "    pages_text: list[dict],\n",
    "    company_name: str | None,\n",
    "    toc_start_page: int,\n",
    "    toc_end_page: int,\n",
    "    company_folder: str | None = None,\n",
    "    min_word_count: int = 1000,\n",
    ") -> dict:\n",
    "    max_pdf_page = len(pages_text)\n",
    "    start_hint = int(toc_start_page)\n",
    "    end_hint = int(toc_end_page)\n",
    "\n",
    "    toc_next_title, toc_next_page_hint = _find_next_section_hint_from_toc(pages_text, after_page=end_hint)\n",
    "\n",
    "    toc_mdna_title = _find_section_title_for_page_from_toc(pages_text, target_page=start_hint)\n",
    "\n",
    "    stop_phrases = [\n",
    "        toc_next_title or \"\",\n",
    "        \"INDEPENDENT AUDITOR\",\n",
    "        \"INDEPENDENT AUDITORS REPORT\",\n",
    "        \"AUDITOR'S REPORT\",\n",
    "        \"AUDITORS' REPORT\",\n",
    "        \"AUDITORS REPORT\",\n",
    "        \"FINANCIAL STATEMENTS\",\n",
    "        \"STANDALONE FINANCIAL STATEMENTS\",\n",
    "        \"CORPORATE GOVERNANCE\",\n",
    "        \"BALANCE SHEET\",\n",
    "        \"NOTES\",\n",
    "    ]\n",
    "\n",
    "    start_title_candidates = [\n",
    "        toc_mdna_title or \"\",\n",
    "        \"Management Discussion and Analysis\",\n",
    "        \"Management Discussion & Analysis\",\n",
    "        \"Management Discussion\",\n",
    "    ]\n",
    "    start_title_candidates = [t for t in start_title_candidates if t]\n",
    "\n",
    "    candidates: dict[int, int] = {}\n",
    "    for title in start_title_candidates:\n",
    "        for page, score in _find_candidate_pages_by_header_match(\n",
    "            doc,\n",
    "            title,\n",
    "            start_hint,\n",
    "            window=5,\n",
    "            header_ratio=0.20,\n",
    "            min_score=85,\n",
    "        ):\n",
    "            candidates[page] = max(candidates.get(page, -1), score)\n",
    "\n",
    "    candidate_pages = sorted(candidates.keys())\n",
    "\n",
    "    strong_heading_re = re.compile(\n",
    "        r\"(?im)^\\s*(?:\\(?\\d+\\)?\\s*[\\).:-]\\s*)?(?:\"\n",
    "        r\"management\\s+discussion(?:\\s+(?:and|&)\\s+analysis)?\"\n",
    "        r\"|md\\s*&\\s*a\"\n",
    "        r\"|financial\\s+analysis\\s+and\\s+performance\\s+review\"\n",
    "        r\")\\b\"\n",
    "    )\n",
    "    weak_phrase_re = re.compile(\n",
    "        r\"(?i)\\b(?:management\\s+discussion(?:\\s*(?:and|&)\\s*analysis)?\"\n",
    "        r\"|md\\s*&\\s*a\"\n",
    "        r\"|financial\\s+analysis\\s+and\\s+performance\\s+review)\\b\"\n",
    "    )\n",
    "\n",
    "    forward_candidates = _find_candidate_pages_by_top_lines(\n",
    "        pages_text,\n",
    "        start_page=start_hint,\n",
    "        max_forward=250,\n",
    "        pattern=strong_heading_re,\n",
    "        max_lines=150,\n",
    "    )\n",
    "    if not forward_candidates:\n",
    "        forward_candidates = _find_candidate_pages_by_top_lines(\n",
    "            pages_text,\n",
    "            start_page=start_hint,\n",
    "            max_forward=250,\n",
    "            pattern=weak_phrase_re,\n",
    "            max_lines=220,\n",
    "        )\n",
    "\n",
    "    candidate_pages = sorted(dict.fromkeys(candidate_pages + forward_candidates))\n",
    "\n",
    "    def _compute_end_page(start_page_1based: int) -> tuple[int, str, str | None]:\n",
    "        found_next_pages: list[tuple[int, str]] = []\n",
    "        stop_title_candidates = [\n",
    "            toc_next_title,\n",
    "            \"Corporate Governance\",\n",
    "            \"Independent Auditor\",\n",
    "            \"Auditor's Report\",\n",
    "            \"Auditors Report\",\n",
    "            \"Financial Statements\",\n",
    "            \"Standalone Financial Statements\",\n",
    "            \"Balance Sheet\",\n",
    "            \"Notes\",\n",
    "        ]\n",
    "        stop_title_candidates = [t for t in stop_title_candidates if t]\n",
    "\n",
    "        hint = int(toc_next_page_hint) if isinstance(toc_next_page_hint, int) else int(end_hint + 1)\n",
    "        if hint <= int(start_page_1based):\n",
    "            hint = int(start_page_1based) + 8\n",
    "\n",
    "        for title in stop_title_candidates:\n",
    "            nxt = find_real_page_index(doc, title, hint)\n",
    "            if nxt is None:\n",
    "                continue\n",
    "            if int(nxt) <= int(start_page_1based):\n",
    "                continue\n",
    "            found_next_pages.append((int(nxt), title))\n",
    "\n",
    "        if found_next_pages:\n",
    "            nxt_page, used = min(found_next_pages, key=lambda x: x[0])\n",
    "            return max(start_page_1based, nxt_page - 1), \"visual\", used\n",
    "\n",
    "        if int(end_hint) > int(start_page_1based):\n",
    "            return int(end_hint), \"fallback\", toc_next_title\n",
    "\n",
    "        return min(int(start_page_1based) + 25, max_pdf_page), \"fallback\", toc_next_title\n",
    "\n",
    "    def _try_block(start_page_1based: int) -> tuple[dict | None, str]:\n",
    "        end_page_1based, end_method, used_stop_title = _compute_end_page(start_page_1based)\n",
    "\n",
    "        s = max(1, min(int(start_page_1based), max_pdf_page))\n",
    "        e = max(1, min(int(end_page_1based), max_pdf_page))\n",
    "        if e < s:\n",
    "            e = s\n",
    "\n",
    "        raw = extract_text_range(pages_text, start_page=s, end_page=e, stop_phrases=stop_phrases)\n",
    "        cleaned = _clean_mdna_headers_only(raw, company_name)\n",
    "\n",
    "        effective = cleaned\n",
    "        if (company_folder or \"\").strip().lower() == \"amit_spinning\":\n",
    "            effective = _trim_before_mdna_heading_for_amit(effective)\n",
    "\n",
    "        wc = _count_words(effective)\n",
    "\n",
    "        if wc < int(min_word_count):\n",
    "            return None, f\"too_short(word_count={wc})\"\n",
    "\n",
    "        if _has_disqualifier_near_top(effective):\n",
    "            return None, \"disqualifier_near_top\"\n",
    "\n",
    "        return {\n",
    "            \"mdna_start_page\": s,\n",
    "            \"mdna_end_page\": e,\n",
    "            \"mdna_text\": effective,\n",
    "            \"start_method\": \"visual\",\n",
    "            \"end_method\": end_method,\n",
    "            \"boundaries_confident\": bool(end_method == \"visual\"),\n",
    "            \"mdna_title_used\": toc_mdna_title or \"Management Discussion\",\n",
    "            \"next_title_used\": used_stop_title,\n",
    "            \"word_count\": wc,\n",
    "        }, \"ok\"\n",
    "\n",
    "    for cand_start in candidate_pages:\n",
    "        block, reason = _try_block(cand_start)\n",
    "        if block is not None:\n",
    "            return block\n",
    "        logging.warning(\"Rejected candidate start page %s: %s\", cand_start, reason)\n",
    "\n",
    "    solver = PageOffsetSolver(doc, pages_text=pages_text, toc_max_pages=15)\n",
    "    offset = solver.find_offset()\n",
    "\n",
    "    real_start = max(1, min(int(start_hint + offset), max_pdf_page))\n",
    "    real_end = max(1, min(int(end_hint + offset), max_pdf_page))\n",
    "    if real_end < real_start:\n",
    "        real_end = real_start\n",
    "\n",
    "    raw = extract_text_range(pages_text, start_page=real_start, end_page=real_end, stop_phrases=stop_phrases)\n",
    "    cleaned = _clean_mdna_headers_only(raw, company_name)\n",
    "\n",
    "    effective = cleaned\n",
    "    if (company_folder or \"\").strip().lower() == \"amit_spinning\":\n",
    "        effective = _trim_before_mdna_heading_for_amit(effective)\n",
    "\n",
    "    wc = _count_words(effective)\n",
    "\n",
    "    if wc < int(min_word_count) or _has_disqualifier_near_top(effective):\n",
    "        return {\n",
    "            \"mdna_start_page\": real_start,\n",
    "            \"mdna_end_page\": real_end,\n",
    "            \"mdna_text\": \"\",\n",
    "            \"start_method\": \"fallback\",\n",
    "            \"end_method\": \"fallback\",\n",
    "            \"boundaries_confident\": False,\n",
    "            \"mdna_title_used\": toc_mdna_title or \"Management Discussion\",\n",
    "            \"next_title_used\": toc_next_title,\n",
    "            \"word_count\": 0,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"mdna_start_page\": real_start,\n",
    "        \"mdna_end_page\": real_end,\n",
    "        \"mdna_text\": effective,\n",
    "        \"start_method\": \"fallback\",\n",
    "        \"end_method\": \"fallback\",\n",
    "        \"boundaries_confident\": False,\n",
    "        \"mdna_title_used\": toc_mdna_title or \"Management Discussion\",\n",
    "        \"next_title_used\": toc_next_title,\n",
    "        \"word_count\": wc,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26992cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-03 13:52:55,474 - INFO - Discovered 14 PDFs under ..\\data\\pdfs\n",
      "2026-01-03 13:52:55,478 - INFO - (1/14) Processing: Alcheimist/5267070319.pdf\n",
      "2026-01-03 13:52:55,483 - INFO - Loaded PDF: 5267070319.pdf\n",
      "2026-01-03 13:52:55,858 - INFO - Extracted company name: ALCHEMIST LTD\n",
      "2026-01-03 13:52:55,859 - INFO - Extracted financial year: 2018-19\n",
      "2026-01-03 13:52:55,860 - WARNING - Excluded section found but page number not aligned; ignoring exclusion: DIRECTORS’ REPORT\n",
      "2026-01-03 13:52:55,861 - INFO - MD&A boundaries (STRICT ToC blocks): start=23, end=100\n",
      "2026-01-03 13:52:56,138 - INFO - MD&A quality — word_count: 32968\n",
      "2026-01-03 13:52:56,139 - INFO - MD&A quality — narrative_density: 0.7227\n",
      "2026-01-03 13:52:56,140 - INFO - MD&A quality — keyword_hits (2): ['risk management', 'segment performance']\n",
      "2026-01-03 13:52:56,141 - INFO - MD&A quality — pages_match_toc: True\n",
      "2026-01-03 13:52:56,141 - INFO - MD&A quality — disqualifier_hit: True\n",
      "2026-01-03 13:52:56,142 - WARNING - MD&A quality FLAGGED (pages_match_toc=True, phrases_ok=True, disqualifier_hit=True)\n",
      "2026-01-03 13:52:56,144 - INFO - (2/14) Processing: Alcheimist/67050526707.pdf\n",
      "2026-01-03 13:52:56,146 - INFO - Loaded PDF: 67050526707.pdf\n",
      "2026-01-03 13:52:56,438 - INFO - Extracted company name: ALCHEMIST LIMITED\n",
      "2026-01-03 13:52:56,439 - INFO - Extracted financial year: 2019-20\n",
      "2026-01-03 13:52:56,441 - INFO - MD&A boundaries (STRICT ToC blocks): start=29, end=31\n",
      "2026-01-03 13:52:56,740 - INFO - MD&A quality — word_count: 3402\n",
      "2026-01-03 13:52:56,741 - INFO - MD&A quality — narrative_density: 0.8195\n",
      "2026-01-03 13:52:56,742 - INFO - MD&A quality — keyword_hits (0): []\n",
      "2026-01-03 13:52:56,742 - INFO - MD&A quality — pages_match_toc: True\n",
      "2026-01-03 13:52:56,743 - INFO - MD&A quality — disqualifier_hit: False\n",
      "2026-01-03 13:52:56,744 - WARNING - MD&A quality FLAGGED (pages_match_toc=True, phrases_ok=False, disqualifier_hit=False)\n",
      "2026-01-03 13:52:56,745 - INFO - (3/14) Processing: ALok/5210700315.pdf\n",
      "2026-01-03 13:52:56,750 - INFO - Loaded PDF: 5210700315.pdf\n",
      "2026-01-03 13:52:57,331 - WARNING - Company name not found in header region; using folder name: ALok\n",
      "2026-01-03 13:52:57,332 - INFO - Financial year not found in first 5 pages\n",
      "2026-01-03 13:52:57,333 - INFO - MD&A boundaries (STRICT ToC blocks): start=51, end=82\n",
      "2026-01-03 13:52:57,595 - WARNING - Rejected candidate start page 56: disqualifier_near_top\n",
      "2026-01-03 13:52:57,787 - INFO - MD&A quality — word_count: 15167\n",
      "2026-01-03 13:52:57,788 - INFO - MD&A quality — narrative_density: 0.7516\n",
      "2026-01-03 13:52:57,789 - INFO - MD&A quality — keyword_hits (0): []\n",
      "2026-01-03 13:52:57,789 - INFO - MD&A quality — pages_match_toc: True\n",
      "2026-01-03 13:52:57,790 - INFO - MD&A quality — disqualifier_hit: False\n",
      "2026-01-03 13:52:57,790 - WARNING - MD&A quality FLAGGED (pages_match_toc=True, phrases_ok=False, disqualifier_hit=False)\n",
      "2026-01-03 13:52:57,793 - INFO - (4/14) Processing: ALok/5210700316.pdf\n",
      "2026-01-03 13:52:57,796 - INFO - Loaded PDF: 5210700316.pdf\n",
      "2026-01-03 13:52:58,199 - WARNING - Company name not found in header region; using folder name: ALok\n",
      "2026-01-03 13:52:58,200 - INFO - Extracted financial year: 2015-16\n",
      "2026-01-03 13:52:58,201 - INFO - MD&A boundaries (STRICT ToC blocks): start=46, end=62\n",
      "2026-01-03 13:52:58,753 - INFO - MD&A quality — word_count: 12338\n",
      "2026-01-03 13:52:58,754 - INFO - MD&A quality — narrative_density: 0.7543\n",
      "2026-01-03 13:52:58,755 - INFO - MD&A quality — keyword_hits (1): ['risk management']\n",
      "2026-01-03 13:52:58,755 - INFO - MD&A quality — pages_match_toc: True\n",
      "2026-01-03 13:52:58,756 - INFO - MD&A quality — disqualifier_hit: False\n",
      "2026-01-03 13:52:58,756 - WARNING - MD&A quality FLAGGED (pages_match_toc=True, phrases_ok=False, disqualifier_hit=False)\n",
      "2026-01-03 13:52:58,758 - INFO - (5/14) Processing: ALok/5210700317.pdf\n",
      "2026-01-03 13:52:58,763 - INFO - Loaded PDF: 5210700317.pdf\n",
      "2026-01-03 13:52:59,639 - WARNING - Company name not found in header region; using folder name: ALok\n",
      "2026-01-03 13:52:59,641 - INFO - Extracted financial year: 2016-17\n",
      "2026-01-03 13:52:59,645 - INFO - MD&A boundaries (STRICT ToC blocks): start=9, end=9\n",
      "2026-01-03 13:53:00,171 - INFO - MD&A quality — word_count: 13124\n",
      "2026-01-03 13:53:00,172 - INFO - MD&A quality — narrative_density: 0.7345\n",
      "2026-01-03 13:53:00,174 - INFO - MD&A quality — keyword_hits (2): ['risk management', 'global economy']\n",
      "2026-01-03 13:53:00,176 - INFO - MD&A quality — pages_match_toc: False\n",
      "2026-01-03 13:53:00,177 - INFO - MD&A quality — disqualifier_hit: False\n",
      "2026-01-03 13:53:00,178 - WARNING - MD&A quality FLAGGED (pages_match_toc=False, phrases_ok=True, disqualifier_hit=False)\n",
      "2026-01-03 13:53:00,182 - INFO - (6/14) Processing: ALok/5210700318.pdf\n",
      "2026-01-03 13:53:00,193 - INFO - Loaded PDF: 5210700318.pdf\n",
      "2026-01-03 13:53:00,939 - WARNING - Company name not found in header region; using folder name: ALok\n",
      "2026-01-03 13:53:00,940 - INFO - Extracted financial year: 2017-18\n",
      "2026-01-03 13:53:00,941 - INFO - MD&A boundaries (STRICT ToC blocks): start=46, end=62\n",
      "2026-01-03 13:53:01,351 - WARNING - Rejected candidate start page 49: disqualifier_near_top\n",
      "2026-01-03 13:53:01,872 - INFO - MD&A quality — word_count: 1887\n",
      "2026-01-03 13:53:01,874 - INFO - MD&A quality — narrative_density: 0.8065\n",
      "2026-01-03 13:53:01,875 - INFO - MD&A quality — keyword_hits (0): []\n",
      "2026-01-03 13:53:01,876 - INFO - MD&A quality — pages_match_toc: True\n",
      "2026-01-03 13:53:01,878 - INFO - MD&A quality — disqualifier_hit: True\n",
      "2026-01-03 13:53:01,879 - WARNING - MD&A quality FLAGGED (pages_match_toc=True, phrases_ok=False, disqualifier_hit=True)\n",
      "2026-01-03 13:53:01,885 - INFO - (7/14) Processing: Amit_spinning/5210760315.pdf\n",
      "2026-01-03 13:53:01,891 - INFO - Loaded PDF: 5210760315.pdf\n",
      "2026-01-03 13:53:02,062 - INFO - Extracted company name: AMIT SPINNING INDUSTRIES LIMITED\n",
      "2026-01-03 13:53:02,064 - INFO - Extracted financial year: 2014-15\n",
      "2026-01-03 13:53:02,070 - INFO - MD&A boundaries (STRICT ToC blocks): start=8, end=15\n",
      "2026-01-03 13:53:02,251 - INFO - MD&A quality — word_count: 5897\n",
      "2026-01-03 13:53:02,254 - INFO - MD&A quality — narrative_density: 0.7651\n",
      "2026-01-03 13:53:02,255 - INFO - MD&A quality — keyword_hits (2): ['risk management', 'future outlook']\n",
      "2026-01-03 13:53:02,256 - INFO - MD&A quality — pages_match_toc: False\n",
      "2026-01-03 13:53:02,257 - INFO - MD&A quality — disqualifier_hit: True\n",
      "2026-01-03 13:53:02,259 - WARNING - MD&A quality FLAGGED (pages_match_toc=False, phrases_ok=True, disqualifier_hit=True)\n",
      "2026-01-03 13:53:02,261 - INFO - (8/14) Processing: Amit_spinning/5210760316.pdf\n",
      "2026-01-03 13:53:02,264 - INFO - Loaded PDF: 5210760316.pdf\n",
      "2026-01-03 13:53:02,412 - INFO - Extracted company name: AMIT SPINNING INDUSTRIES LIMITED\n",
      "2026-01-03 13:53:02,413 - INFO - Extracted financial year: 2015-16\n",
      "2026-01-03 13:53:02,414 - INFO - MD&A appears as sub-entry; inheriting parent start page 4 from 'Board's Report including'\n",
      "2026-01-03 13:53:02,416 - INFO - MD&A boundaries (STRICT ToC blocks): start=4, end=16\n",
      "2026-01-03 13:53:02,744 - INFO - MD&A quality — word_count: 4442\n",
      "2026-01-03 13:53:02,745 - INFO - MD&A quality — narrative_density: 0.7904\n",
      "2026-01-03 13:53:02,746 - INFO - MD&A quality — keyword_hits (2): ['risk management', 'future outlook']\n",
      "2026-01-03 13:53:02,747 - INFO - MD&A quality — pages_match_toc: True\n",
      "2026-01-03 13:53:02,748 - INFO - MD&A quality — disqualifier_hit: True\n",
      "2026-01-03 13:53:02,749 - WARNING - MD&A quality FLAGGED (pages_match_toc=True, phrases_ok=True, disqualifier_hit=True)\n",
      "2026-01-03 13:53:02,750 - INFO - (9/14) Processing: Amit_spinning/5210760317.pdf\n",
      "2026-01-03 13:53:02,752 - INFO - Loaded PDF: 5210760317.pdf\n",
      "2026-01-03 13:53:02,867 - WARNING - Falling back to default company name for Amit_spinning\n",
      "2026-01-03 13:53:02,869 - INFO - Extracted financial year: 2016-17\n",
      "2026-01-03 13:53:02,869 - WARNING - MD&A start page not found within strict 3-line window; skipping\n",
      "2026-01-03 13:53:02,870 - INFO - Amit_spinning MD&A boundaries (INDEX): start=3, end=28\n",
      "2026-01-03 13:53:03,191 - INFO - MD&A quality — word_count: 24327\n",
      "2026-01-03 13:53:03,192 - INFO - MD&A quality — narrative_density: 0.7271\n",
      "2026-01-03 13:53:03,193 - INFO - MD&A quality — keyword_hits (1): ['risk management']\n",
      "2026-01-03 13:53:03,195 - INFO - MD&A quality — pages_match_toc: True\n",
      "2026-01-03 13:53:03,197 - INFO - MD&A quality — disqualifier_hit: True\n",
      "2026-01-03 13:53:03,198 - WARNING - MD&A quality FLAGGED (pages_match_toc=True, phrases_ok=False, disqualifier_hit=True)\n",
      "2026-01-03 13:53:03,202 - INFO - (10/14) Processing: Amit_spinning/5210760318.pdf\n",
      "2026-01-03 13:53:03,208 - INFO - Loaded PDF: 5210760318.pdf\n",
      "2026-01-03 13:53:03,318 - INFO - Extracted company name: AMIT SPINNING INDUSTRIES LTD\n",
      "2026-01-03 13:53:03,319 - INFO - Extracted financial year: 2017-18\n",
      "2026-01-03 13:53:03,320 - WARNING - MD&A start page not found within strict 3-line window; skipping\n",
      "2026-01-03 13:53:03,321 - INFO - Amit_spinning MD&A boundaries (INDEX): start=1, end=21\n",
      "2026-01-03 13:53:03,493 - INFO - MD&A quality — word_count: 10086\n",
      "2026-01-03 13:53:03,494 - INFO - MD&A quality — narrative_density: 0.7770\n",
      "2026-01-03 13:53:03,495 - INFO - MD&A quality — keyword_hits (1): ['risk management']\n",
      "2026-01-03 13:53:03,497 - INFO - MD&A quality — pages_match_toc: True\n",
      "2026-01-03 13:53:03,497 - INFO - MD&A quality — disqualifier_hit: True\n",
      "2026-01-03 13:53:03,498 - WARNING - MD&A quality FLAGGED (pages_match_toc=True, phrases_ok=False, disqualifier_hit=True)\n",
      "2026-01-03 13:53:03,498 - INFO - (11/14) Processing: Amtek/5200770316.pdf\n",
      "2026-01-03 13:53:03,500 - INFO - Loaded PDF: 5200770316.pdf\n",
      "2026-01-03 13:53:03,800 - INFO - Extracted company name: AMTEK AUTO LIMITED\n",
      "2026-01-03 13:53:03,801 - INFO - Extracted financial year: 2015-16\n",
      "2026-01-03 13:53:03,804 - INFO - MD&A boundaries (STRICT ToC blocks): start=63, end=71\n",
      "2026-01-03 13:53:04,059 - INFO - MD&A quality — word_count: 3756\n",
      "2026-01-03 13:53:04,060 - INFO - MD&A quality — narrative_density: 0.8019\n",
      "2026-01-03 13:53:04,061 - INFO - MD&A quality — keyword_hits (2): ['risk management', 'global economy']\n",
      "2026-01-03 13:53:04,061 - INFO - MD&A quality — pages_match_toc: True\n",
      "2026-01-03 13:53:04,062 - INFO - MD&A quality — disqualifier_hit: False\n",
      "2026-01-03 13:53:04,062 - INFO - MD&A quality PASSED\n",
      "2026-01-03 13:53:04,064 - INFO - (12/14) Processing: Amtek/5200770317.pdf\n",
      "2026-01-03 13:53:04,065 - INFO - Loaded PDF: 5200770317.pdf\n",
      "2026-01-03 13:53:04,457 - INFO - Extracted company name: AMTEK AUTO LIMITED\n",
      "2026-01-03 13:53:04,458 - INFO - Extracted financial year: 2016-17\n",
      "2026-01-03 13:53:04,460 - INFO - MD&A boundaries (STRICT ToC blocks): start=64, end=72\n",
      "2026-01-03 13:53:04,759 - INFO - MD&A quality — word_count: 4852\n",
      "2026-01-03 13:53:04,760 - INFO - MD&A quality — narrative_density: 0.8038\n",
      "2026-01-03 13:53:04,761 - INFO - MD&A quality — keyword_hits (0): []\n",
      "2026-01-03 13:53:04,761 - INFO - MD&A quality — pages_match_toc: True\n",
      "2026-01-03 13:53:04,762 - INFO - MD&A quality — disqualifier_hit: True\n",
      "2026-01-03 13:53:04,762 - WARNING - MD&A quality FLAGGED (pages_match_toc=True, phrases_ok=False, disqualifier_hit=True)\n",
      "2026-01-03 13:53:04,764 - INFO - (13/14) Processing: Amtek/5200770318.pdf\n",
      "2026-01-03 13:53:04,766 - INFO - Loaded PDF: 5200770318.pdf\n",
      "2026-01-03 13:53:05,388 - INFO - Extracted company name: AMTEK AUTO LIMITED\n",
      "2026-01-03 13:53:05,389 - INFO - Extracted financial year: 2017-18\n",
      "2026-01-03 13:53:05,391 - INFO - MD&A boundaries (STRICT ToC blocks): start=65, end=74\n",
      "2026-01-03 13:53:05,716 - WARNING - Rejected candidate start page 60: disqualifier_near_top\n",
      "2026-01-03 13:53:05,909 - INFO - MD&A quality — word_count: 3643\n",
      "2026-01-03 13:53:05,910 - INFO - MD&A quality — narrative_density: 0.7942\n",
      "2026-01-03 13:53:05,910 - INFO - MD&A quality — keyword_hits (1): ['global economy']\n",
      "2026-01-03 13:53:05,911 - INFO - MD&A quality — pages_match_toc: True\n",
      "2026-01-03 13:53:05,912 - INFO - MD&A quality — disqualifier_hit: False\n",
      "2026-01-03 13:53:05,913 - WARNING - MD&A quality FLAGGED (pages_match_toc=True, phrases_ok=False, disqualifier_hit=False)\n",
      "2026-01-03 13:53:05,915 - INFO - (14/14) Processing: Amtek/5200770915.pdf\n",
      "2026-01-03 13:53:05,918 - INFO - Loaded PDF: 5200770915.pdf\n",
      "2026-01-03 13:53:06,141 - INFO - Extracted company name: AMTEK AUTO LIMITED\n",
      "2026-01-03 13:53:06,142 - INFO - Extracted financial year: 2014-15\n",
      "2026-01-03 13:53:06,147 - INFO - MD&A boundaries (STRICT ToC blocks): start=56, end=63\n",
      "2026-01-03 13:53:06,433 - INFO - MD&A quality — word_count: 2833\n",
      "2026-01-03 13:53:06,435 - INFO - MD&A quality — narrative_density: 0.8161\n",
      "2026-01-03 13:53:06,436 - INFO - MD&A quality — keyword_hits (2): ['risk management', 'global economy']\n",
      "2026-01-03 13:53:06,437 - INFO - MD&A quality — pages_match_toc: True\n",
      "2026-01-03 13:53:06,438 - INFO - MD&A quality — disqualifier_hit: False\n",
      "2026-01-03 13:53:06,438 - INFO - MD&A quality PASSED\n",
      "2026-01-03 13:53:06,491 - INFO - Saved CSV: c:\\Users\\LOQ\\Desktop\\SPJIMR\\mdna_extraction_project\\notebooks\\..\\output\\mdna_extracted.csv\n",
      "2026-01-03 13:53:06,611 - INFO - Saved Excel: c:\\Users\\LOQ\\Desktop\\SPJIMR\\mdna_extraction_project\\notebooks\\..\\output\\mdna_extracted.xlsx\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------\n",
    "# End-to-end MD&A Extraction Pipeline (Visual Anchor Search)\n",
    "# -----------------------------\n",
    "PDF_ROOT = Path(\"../data/pdfs\")\n",
    "\n",
    "# Prefer the configured OUTPUT_DIR if present; otherwise default to ../output\n",
    "try:\n",
    "    output_dir = OUTPUT_DIR\n",
    "except NameError:\n",
    "    output_dir = Path(\"../output\")\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pdf_paths = sorted(PDF_ROOT.rglob(\"*.pdf\"))\n",
    "logging.info(\"Discovered %d PDFs under %s\", len(pdf_paths), PDF_ROOT)\n",
    "\n",
    "results = []\n",
    "\n",
    "attempted = 0\n",
    "succeeded = 0\n",
    "skipped_no_boundaries = 0\n",
    "failed = 0\n",
    "passed_quality = 0\n",
    "flagged_quality = 0\n",
    "\n",
    "for idx, pdf_path in enumerate(pdf_paths, start=1):\n",
    "    attempted += 1\n",
    "    company_folder = pdf_path.parent.name\n",
    "\n",
    "    logging.info(\"(%d/%d) Processing: %s/%s\", idx, len(pdf_paths), company_folder, pdf_path.name)\n",
    "\n",
    "    pdf = None\n",
    "    try:\n",
    "        pdf = PDFInterface(pdf_path)\n",
    "        pages = pdf.get_pages_text()\n",
    "\n",
    "        company_name = extract_company_name(pages, company_folder=company_folder)\n",
    "        financial_year = extract_financial_year(pages)\n",
    "\n",
    "        # ToC detection stays as-is (working logic); we only change mapping/extraction core.\n",
    "        toc_start_page, toc_end_page = detect_mdna_boundaries(\n",
    "            pages_text=pages,\n",
    "            toc_start_page=None,\n",
    "            company_folder=company_folder,\n",
    "        )\n",
    "\n",
    "        if not toc_start_page or not toc_end_page:\n",
    "            skipped_no_boundaries += 1\n",
    "            logging.warning(\"Skipping (MD&A boundaries not determinable via STRICT ToC rules): %s\", pdf_path.name)\n",
    "            continue\n",
    "\n",
    "        block = extract_mdna_block(\n",
    "            doc=pdf.doc,\n",
    "            pages_text=pages,\n",
    "            company_name=company_name,\n",
    "            toc_start_page=int(toc_start_page),\n",
    "            toc_end_page=int(toc_end_page),\n",
    "            company_folder=company_folder,\n",
    "        )\n",
    "\n",
    "        cleaned_mdna_text = block[\"mdna_text\"]\n",
    "\n",
    "        # Quality: treat visually-confirmed boundaries as 'pages_match_toc' confidence.\n",
    "        quality_report = verify_mdna_quality(\n",
    "            cleaned_mdna_text,\n",
    "            pages_match_toc=bool(block.get(\"boundaries_confident\")),\n",
    "        )\n",
    "\n",
    "        if quality_report.get(\"quality_passed\"):\n",
    "            passed_quality += 1\n",
    "        else:\n",
    "            flagged_quality += 1\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"company_folder\": company_folder,\n",
    "                \"company_name\": company_name,\n",
    "                \"report_file\": pdf_path.name,\n",
    "                \"financial_year\": financial_year,\n",
    "                \"mdna_start_page\": block[\"mdna_start_page\"],\n",
    "                \"mdna_end_page\": block[\"mdna_end_page\"],\n",
    "                \"mdna_text\": cleaned_mdna_text,\n",
    "                **quality_report,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        succeeded += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        logging.exception(\"Failed processing %s: %s\", pdf_path, e)\n",
    "\n",
    "    finally:\n",
    "        if pdf is not None:\n",
    "            pdf.close()\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# -----------------------------\n",
    "# Safe CSV/Excel writes (avoid Windows PermissionError when file is open)\n",
    "# -----------------------------\n",
    "run_stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "csv_path = output_dir / \"mdna_extracted.csv\"\n",
    "xlsx_path = output_dir / \"mdna_extracted.xlsx\"\n",
    "\n",
    "fallback_csv = output_dir / f\"mdna_extracted_{run_stamp}.csv\"\n",
    "fallback_xlsx = output_dir / f\"mdna_extracted_{run_stamp}.xlsx\"\n",
    "\n",
    "# Excel safety: remove illegal control characters\n",
    "def _sanitize_for_excel(val):\n",
    "    if val is None:\n",
    "        return \"\"\n",
    "    if isinstance(val, (list, dict, tuple, set)):\n",
    "        val = str(val)\n",
    "    s = str(val)\n",
    "    return re.sub(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]\", \"\", s)\n",
    "\n",
    "# Exclude quality_passed and keyword_hits from output files\n",
    "out_cols = [col for col in results_df.columns if col not in ['quality_passed', 'keyword_hits']]\n",
    "\n",
    "excel_df = results_df[out_cols].copy()\n",
    "for col in excel_df.columns:\n",
    "    excel_df[col] = excel_df[col].map(_sanitize_for_excel)\n",
    "\n",
    "try:\n",
    "    results_df[out_cols].to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "    logging.info(\"Saved CSV: %s\", csv_path)\n",
    "except PermissionError:\n",
    "    results_df[out_cols].to_csv(fallback_csv, index=False, encoding=\"utf-8\")\n",
    "    logging.warning(\"CSV locked; saved fallback CSV: %s\", fallback_csv)\n",
    "\n",
    "try:\n",
    "    excel_df.to_excel(xlsx_path, index=False)\n",
    "    logging.info(\"Saved Excel: %s\", xlsx_path)\n",
    "except PermissionError:\n",
    "    excel_df.to_excel(fallback_xlsx, index=False)\n",
    "    logging.warning(\"Excel locked; saved fallback Excel: %s\", fallback_xlsx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c7a9d",
   "metadata": {},
   "source": [
    "### 12. Pipeline Output Summary :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87c53bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline output summary\n",
      "- results_df shape: (14, 13)\n",
      "- quality passed: 2\n",
      "- quality flagged: 12\n"
     ]
    }
   ],
   "source": [
    "# Quick summary of how many rows were extracted\n",
    "print(\"\\nPipeline output summary\")\n",
    "print(\"- results_df shape:\", results_df.shape)\n",
    "print(\"- quality passed:\", int((results_df[\"quality_passed\"] == True).sum()) if \"quality_passed\" in results_df.columns else \"N/A\")\n",
    "print(\"- quality flagged:\", int((results_df[\"quality_passed\"] == False).sum()) if \"quality_passed\" in results_df.columns else \"N/A\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720dc3e3",
   "metadata": {},
   "source": [
    "### 13. MD&A Extraction Diagnostics :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3cd9bb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MD&A extraction diagnostics\n",
      "- has_mdna_heading: 14 / 14\n",
      "- has_structure_or_outlook: 14 / 14\n",
      "- has_audit_terms (should be low): 6 / 14\n",
      "- has_directors_report (should be low): 3 / 14\n",
      "- keyword_hits_count>=2 (required by current quality gate): 6 / 14\n",
      "\n",
      "Sample rows (first 10):\n",
      "company_folder     report_file financial_year  mdna_start_page  mdna_end_page  word_count  narrative_density                           keyword_hits  has_mdna_heading  has_structure_or_outlook  has_audit_terms  has_directors_report\n",
      "    Alcheimist  5267070319.pdf        2018-19               26             95       32968           0.722728 [risk management, segment performance]              True                      True             True                 False\n",
      "    Alcheimist 67050526707.pdf        2019-20               29             32        3402           0.819517                                     []              True                      True            False                 False\n",
      "          ALok  5210700315.pdf           None               57             81       15167           0.751624                                     []              True                      True            False                 False\n",
      "          ALok  5210700316.pdf        2015-16               49             66       12338           0.754301                      [risk management]              True                      True            False                 False\n",
      "          ALok  5210700317.pdf        2016-17               64             89       13124           0.734460      [risk management, global economy]              True                      True            False                 False\n",
      "          ALok  5210700318.pdf        2017-18               65             67        1887           0.806525                                     []              True                      True            False                 False\n",
      " Amit_spinning  5210760315.pdf        2014-15                8             15        5897           0.765103      [risk management, future outlook]              True                      True             True                  True\n",
      " Amit_spinning  5210760316.pdf        2015-16                7             12        4442           0.790423      [risk management, future outlook]              True                      True             True                 False\n",
      " Amit_spinning  5210760317.pdf        2016-17                6             46       24327           0.727093                      [risk management]              True                      True             True                  True\n",
      " Amit_spinning  5210760318.pdf        2017-18                4             19       10086           0.777000                      [risk management]              True                      True             True                  True\n"
     ]
    }
   ],
   "source": [
    "# Diagnostics: sanity-check extracted MD&A text looks like MD&A (not Audit/Directors)\n",
    "import re\n",
    "\n",
    "if \"results_df\" not in globals() or results_df is None or results_df.empty:\n",
    "    print(\"results_df is empty; run the pipeline cell first\")\n",
    "else:\n",
    "    df = results_df.copy()\n",
    "\n",
    "    def _contains(pat: str, s: str) -> bool:\n",
    "        return bool(re.search(pat, s or \"\", flags=re.IGNORECASE))\n",
    "\n",
    "    df[\"has_mdna_heading\"] = df[\"mdna_text\"].map(lambda s: _contains(r\"management\\s+discussion\", s))\n",
    "    df[\"has_structure_or_outlook\"] = df[\"mdna_text\"].map(lambda s: _contains(r\"\\bstructure\\b|\\boutlook\\b\", s))\n",
    "    df[\"has_audit_terms\"] = df[\"mdna_text\"].map(lambda s: _contains(r\"independent\\s+auditor|auditors?\\s*[’']?\\s*report\", s))\n",
    "    df[\"has_directors_report\"] = df[\"mdna_text\"].map(lambda s: _contains(r\"directors?\\s*[’']?\\s*report\", s))\n",
    "    df[\"keyword_hits_count\"] = df[\"keyword_hits\"].map(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "\n",
    "    print(\"\\nMD&A extraction diagnostics\")\n",
    "    print(\"- has_mdna_heading:\", int(df[\"has_mdna_heading\"].sum()), \"/\", len(df))\n",
    "    print(\"- has_structure_or_outlook:\", int(df[\"has_structure_or_outlook\"].sum()), \"/\", len(df))\n",
    "    print(\"- has_audit_terms (should be low):\", int(df[\"has_audit_terms\"].sum()), \"/\", len(df))\n",
    "    print(\"- has_directors_report (should be low):\", int(df[\"has_directors_report\"].sum()), \"/\", len(df))\n",
    "    print(\"- keyword_hits_count>=2 (required by current quality gate):\", int((df[\"keyword_hits_count\"] >= 2).sum()), \"/\", len(df))\n",
    "\n",
    "    show_cols = [\n",
    "        \"company_folder\",\n",
    "        \"report_file\",\n",
    "        \"financial_year\",\n",
    "        \"mdna_start_page\",\n",
    "        \"mdna_end_page\",\n",
    "        \"word_count\",\n",
    "        \"narrative_density\",\n",
    "        \"keyword_hits\",\n",
    "        \"has_mdna_heading\",\n",
    "        \"has_structure_or_outlook\",\n",
    "        \"has_audit_terms\",\n",
    "        \"has_directors_report\",\n",
    "    ]\n",
    "    show_cols = [c for c in show_cols if c in df.columns]\n",
    "\n",
    "    print(\"\\nSample rows (first 10):\")\n",
    "    print(df[show_cols].head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31bbf4",
   "metadata": {},
   "source": [
    "### 14. Amit_spinning Diagnostic Check :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "005a8e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "results_df columns: ['company_folder', 'company_name', 'report_file', 'financial_year', 'mdna_start_page', 'mdna_end_page', 'mdna_text', 'start_method', 'end_method', 'mdna_title_used', 'next_title_used', 'word_count', 'narrative_density', 'keyword_hits', 'quality_passed']\n",
      "\n",
      "Amit_spinning preview (after running Cell 18):\n",
      "company_folder    report_file financial_year  mdna_start_page  mdna_end_page  word_count  quality_passed\n",
      " Amit_spinning 5210760315.pdf        2014-15                8             15        5897           False\n",
      " Amit_spinning 5210760316.pdf        2015-16                7             12        4442           False\n",
      " Amit_spinning 5210760317.pdf        2016-17                6             46       24327           False\n",
      " Amit_spinning 5210760318.pdf        2017-18                4             19       10086           False\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: confirm Amit_spinning PDFs were processed in the latest run\n",
    "if \"results_df\" in globals() and results_df is not None:\n",
    "    print(\"\\nresults_df columns:\", list(results_df.columns))\n",
    "\n",
    "    company_col = \"company_folder\" if \"company_folder\" in results_df.columns else (\"company\" if \"company\" in results_df.columns else None)\n",
    "    if company_col:\n",
    "        subset = results_df[results_df[company_col] == \"Amit_spinning\"].copy()\n",
    "        print(\"\\nAmit_spinning preview (after running Cell 18):\")\n",
    "        if subset.empty:\n",
    "            print(\"(none)\")\n",
    "        else:\n",
    "            display_cols = [c for c in [\n",
    "                company_col,\n",
    "                \"report_file\",\n",
    "                \"financial_year\",\n",
    "                \"mdna_start_page\",\n",
    "                \"mdna_end_page\",\n",
    "                \"word_count\",\n",
    "                \"quality_passed\",\n",
    "            ] if c in subset.columns]\n",
    "            subset = subset.sort_values([\"financial_year\"] if \"financial_year\" in subset.columns else [company_col])\n",
    "            print(subset[display_cols].to_string(index=False))\n",
    "    else:\n",
    "        print(\"Could not find a company column in results_df\")\n",
    "else:\n",
    "    print(\"results_df not found; run Cell 18 first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e89ac29",
   "metadata": {},
   "source": [
    "### 15. Targeted Quality Spot-Checks :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af4447",
   "metadata": {},
   "source": [
    "#### a) Test cell — MD&A Boundary Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eccabf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:39:33,640 - INFO - Loaded PDF: 5267070319.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MD&A boundary detection on sample PDFs:\n",
      "\n",
      "======================================================================\n",
      "Company Folder : Alcheimist\n",
      "PDF File       : 5267070319.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:39:34,245 - WARNING - Excluded section found but page number not aligned; ignoring exclusion: DIRECTORS’ REPORT\n",
      "2026-01-02 22:39:34,247 - INFO - MD&A boundaries (STRICT ToC blocks): start=23, end=100\n",
      "2026-01-02 22:39:34,254 - INFO - Loaded PDF: 5210700315.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected MD&A Start Page: 23\n",
      "Detected MD&A End Page  : 100\n",
      "✔ Boundary detection looks valid\n",
      "======================================================================\n",
      "Company Folder : ALok\n",
      "PDF File       : 5210700315.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:39:35,286 - INFO - MD&A boundaries (STRICT ToC blocks): start=51, end=82\n",
      "2026-01-02 22:39:35,289 - INFO - Loaded PDF: 5210760318.pdf\n",
      "2026-01-02 22:39:35,393 - WARNING - MD&A start page not found within strict 3-line window; skipping\n",
      "2026-01-02 22:39:35,396 - INFO - Amit_spinning MD&A boundaries (INDEX): start=1, end=21\n",
      "2026-01-02 22:39:35,399 - INFO - Loaded PDF: 5200770316.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected MD&A Start Page: 51\n",
      "Detected MD&A End Page  : 82\n",
      "✔ Boundary detection looks valid\n",
      "======================================================================\n",
      "Company Folder : Amit_spinning\n",
      "PDF File       : 5210760318.pdf\n",
      "Detected MD&A Start Page: 1\n",
      "Detected MD&A End Page  : 21\n",
      "✔ Boundary detection looks valid\n",
      "======================================================================\n",
      "Company Folder : Amtek\n",
      "PDF File       : 5200770316.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:39:35,816 - INFO - MD&A boundaries (STRICT ToC blocks): start=63, end=71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected MD&A Start Page: 63\n",
      "Detected MD&A End Page  : 71\n",
      "✔ Boundary detection looks valid\n",
      "\n",
      "Boundary detection test completed.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PDF_ROOT = Path(\"../data/pdfs\")\n",
    "\n",
    "# Pick ONE representative PDF per company\n",
    "test_pdfs = {}\n",
    "for pdf in PDF_ROOT.rglob(\"*.pdf\"):\n",
    "    company = pdf.parent.name\n",
    "    if company not in test_pdfs:\n",
    "        test_pdfs[company] = pdf\n",
    "\n",
    "# Prefer an INDEX-style Amit_spinning file that exercises the special-case logic\n",
    "amit_preferred = PDF_ROOT / \"Amit_spinning\" / \"5210760318.pdf\"\n",
    "if amit_preferred.exists():\n",
    "    test_pdfs[\"Amit_spinning\"] = amit_preferred\n",
    "\n",
    "print(\"Testing MD&A boundary detection on sample PDFs:\\n\")\n",
    "\n",
    "for company, pdf_path in test_pdfs.items():\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Company Folder : {company}\")\n",
    "    print(f\"PDF File       : {pdf_path.name}\")\n",
    "\n",
    "    pdf = PDFInterface(pdf_path)\n",
    "    pages = pdf.get_pages_text()\n",
    "\n",
    "    start_page, end_page = detect_mdna_boundaries(\n",
    "        pages_text=pages,\n",
    "        toc_start_page=None,\n",
    "        company_folder=company,\n",
    "    )\n",
    "\n",
    "    print(f\"Detected MD&A Start Page: {start_page}\")\n",
    "    print(f\"Detected MD&A End Page  : {end_page}\")\n",
    "\n",
    "    if start_page and end_page:\n",
    "        assert start_page <= end_page, \"Start page must be strictly before end page\"\n",
    "        assert 1 <= start_page <= len(pages), \"Start page out of range\"\n",
    "        assert 1 <= end_page <= len(pages), \"End page out of range\"\n",
    "        print(\"✔ Boundary detection looks valid\")\n",
    "    else:\n",
    "        print(\"⚠ MD&A boundaries not detected (may require fallback logic)\")\n",
    "\n",
    "print(\"\\nBoundary detection test completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802dd04d",
   "metadata": {},
   "source": [
    "#### b) Sanity check  after cell 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38ebeec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:39:35,836 - INFO - Loaded PDF: 5267070319.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\LOQ\\Desktop\\SPJIMR\\mdna_extraction_project\\notebooks\n",
      "PDF_ROOT: ..\\data\\pdfs\n",
      "PDF_ROOT exists: True\n",
      "PDF_ROOT resolved: C:\\Users\\LOQ\\Desktop\\SPJIMR\\mdna_extraction_project\\data\\pdfs\n",
      "Total PDFs found: 14\n",
      "\n",
      "Sample PDFs selected for testing:\n",
      "- Alcheimist: 5267070319.pdf\n",
      "- ALok: 5210700315.pdf\n",
      "- Amit_spinning: 5210760315.pdf\n",
      "- Amtek: 5200770316.pdf\n",
      "\n",
      "--- Running Sanity Checks ---\n",
      "\n",
      "Testing company: Alcheimist\n",
      "PDF: 5267070319.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:39:36,444 - INFO - Extracted company name: ALCHEMIST LTD\n",
      "2026-01-02 22:39:36,445 - INFO - Extracted financial year: 2018-19\n",
      "2026-01-02 22:39:36,452 - INFO - Loaded PDF: 5210700315.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages extracted: 145\n",
      "Extracted Company Name: ALCHEMIST LTD\n",
      "Extracted Financial Year: 2018-19\n",
      "--------------------------------------------------\n",
      "Testing company: ALok\n",
      "PDF: 5210700315.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:39:37,533 - WARNING - Company name not found in header region; using folder name: ALok\n",
      "2026-01-02 22:39:37,534 - INFO - Financial year not found in first 5 pages\n",
      "2026-01-02 22:39:37,540 - INFO - Loaded PDF: 5210760315.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages extracted: 208\n",
      "Extracted Company Name: ALOK\n",
      "Extracted Financial Year: None\n",
      "--------------------------------------------------\n",
      "Testing company: Amit_spinning\n",
      "PDF: 5210760315.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:39:37,872 - INFO - Extracted company name: AMIT SPINNING INDUSTRIES LIMITED\n",
      "2026-01-02 22:39:37,874 - INFO - Extracted financial year: 2014-15\n",
      "2026-01-02 22:39:37,879 - INFO - Loaded PDF: 5200770316.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages extracted: 46\n",
      "Extracted Company Name: AMIT SPINNING INDUSTRIES LIMITED\n",
      "Extracted Financial Year: 2014-15\n",
      "--------------------------------------------------\n",
      "Testing company: Amtek\n",
      "PDF: 5200770316.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 22:39:38,333 - INFO - Extracted company name: AMTEK AUTO LIMITED\n",
      "2026-01-02 22:39:38,334 - INFO - Extracted financial year: 2015-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages extracted: 145\n",
      "Extracted Company Name: AMTEK AUTO LIMITED\n",
      "Extracted Financial Year: 2015-16\n",
      "--------------------------------------------------\n",
      "\n",
      "Sanity check completed successfully.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ---- CONFIG ----\n",
    "PDF_ROOT = Path(\"../data/pdfs\")\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"PDF_ROOT: {PDF_ROOT}\")\n",
    "print(f\"PDF_ROOT exists: {PDF_ROOT.exists()}\")\n",
    "print(f\"PDF_ROOT resolved: {PDF_ROOT.resolve()}\")\n",
    "\n",
    "# ---- STEP 1: Discover PDFs ----\n",
    "pdf_files = list(PDF_ROOT.rglob(\"*.pdf\"))\n",
    "\n",
    "print(f\"Total PDFs found: {len(pdf_files)}\")\n",
    "\n",
    "if not pdf_files:\n",
    "    print(\"No PDFs found in ../data/pdfs directory. Please add PDF files to test the pipeline.\")\n",
    "    print(\"Skipping sanity checks.\")\n",
    "else:\n",
    "    # Pick one PDF from each company (if available)\n",
    "    sample_pdfs = {}\n",
    "    for pdf in pdf_files:\n",
    "        company = pdf.parent.name\n",
    "        if company not in sample_pdfs:\n",
    "            sample_pdfs[company] = pdf\n",
    "\n",
    "    print(\"\\nSample PDFs selected for testing:\")\n",
    "    for company, pdf in sample_pdfs.items():\n",
    "        print(f\"- {company}: {pdf.name}\")\n",
    "\n",
    "    # ---- STEP 2: Test PDFInterface + Metadata Extraction ----\n",
    "    print(\"\\n--- Running Sanity Checks ---\\n\")\n",
    "\n",
    "    for company, pdf_path in sample_pdfs.items():\n",
    "        print(f\"Testing company: {company}\")\n",
    "        print(f\"PDF: {pdf_path.name}\")\n",
    "\n",
    "        pdf = PDFInterface(pdf_path)\n",
    "        pages = pdf.get_pages_text()\n",
    "\n",
    "        print(\"Pages extracted:\", len(pages))\n",
    "        assert len(pages) > 0, \"No pages extracted!\"\n",
    "\n",
    "        # Metadata extraction\n",
    "        extracted_company = extract_company_name(pages, company_folder=company)\n",
    "        extracted_year = extract_financial_year(pages)\n",
    "\n",
    "        print(\"Extracted Company Name:\", extracted_company)\n",
    "        print(\"Extracted Financial Year:\", extracted_year)\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    print(\"\\nSanity check completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0970f69",
   "metadata": {},
   "source": [
    "### c) Targeted quality spot-checks (wrong-start + tail bleed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d36d9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ALok 5210700317 ===\n",
      "file= 5210700317.pdf year= 2016-17 start= 64 end= 89 start_method= visual end_method= fallback word_count= 13124 quality_passed= False\n",
      "text_len= 76497\n",
      "\n",
      "-- head (first ~900 chars) --\n",
      "61 Alok industries is amongst the largest vertically integrated textile companies in India with manufacturing\n",
      "capabilities across both cotton and polyester value chain. Alok has presence in domestic as well as international market\n",
      "through its various subsidiaries and associate concerns. The company has widespread presence and caters to globally\n",
      "renowned brands. Alok has been facing some hurdles over the last few years due to liquidity crunch and a highly\n",
      "leveraged financial position. Despite of all the uncertainties and challenges faced by the company, Alok Industries is\n",
      "optimistic of a resolution and regain its position in the future due to strong fundamentals like technology, manpower,\n",
      "quality, relationship etc. Economic Overview World 2016 was a positive year for the global economy with constant good\n",
      "economic news. The “World Economic Outlook” report by IMF has projected the global ec\n",
      "\n",
      "-- tail (last ~900 chars) --\n",
      " financial restructuring. Success in this front is imperative to stabilise the Company’s position.  Internal Control and\n",
      "Adequacy The Company has in place well established framework of internal control system, commensurate with the size and\n",
      "complexity of its business. The Company has set up processes to continuously monitor the effectiveness of the internal\n",
      "controls with an objective to provide to the Audit Committee and Board of Directors an independent, objective and\n",
      "reasonable assurance on the adequacy and effectiveness of the organization’s risk management, control and governance\n",
      "processes. The Company has a strong and independent internal audit function consisting of professionally qualified\n",
      "accountants with external audit firms monitoring the internal control process at each of the manufacturing location. The\n",
      "Company periodically review accounting guidelines to ensure uniformity of\n",
      "\n",
      "head disqualifiers: []\n",
      "tail bleed indicators: []\n",
      "\n",
      "=== Amit_spinning 5210760315 ===\n",
      "file= 5210760315.pdf year= 2014-15 start= 8 end= 15 start_method= fallback end_method= fallback word_count= 5897 quality_passed= False\n",
      "text_len= 36267\n",
      "\n",
      "-- head (first ~900 chars) --\n",
      "FINANCIAL ANALYSIS AND PERFORMANCE REVIEW During the fiscal 2014-15, the turnover of the Company increased to Rs.\n",
      "3,205.52 Lakhs as compared to Rs. 2,638.57 Lakhs in the previous year. However due to sluggish market, increase in\n",
      "labour, power and other operational costs , and  financial constraints , Company could not optimally utilize its\n",
      "capacities and its lossess increased to Rs. 2,411.71 Lakhs as compared to Rs. 962.02 Lakhs in the previous year.\n",
      "Further, over the period  the company has eroded its net worth completely and it has been declared as a Sick Company\n",
      "under Sick Industrial Companies (Special Provisions) Act'1985  by the  Board for industrial and financial restructuring\n",
      "(BIFR) vide its order dated 18th July'2012. Management Discussion and Analysis Indian Textile Industry is one of the\n",
      "leading textile Industries in the world. The textile sector has always been an important p\n",
      "\n",
      "-- tail (last ~900 chars) --\n",
      " - 2,000 0.00 2,000 - 2,000 0.00 0.00 g) FIIS - - - 0.00 - - - 0.00 0.00 h) Foreign Venture - - - 0.00 - - - 0.00 0.00\n",
      "Capital Funds i) Others (specify) - - - 0.00 - - - 0.00 0.00 Foreign Bank SUB TOTAL (B)(1): 2,000 100 2,100 0.01 2,000\n",
      "100 2,100 0.01 0.00 (2)Non Institutions  a) Bodies Corporates  i) Indian 821,368 7,900 829,268 2.01 860,293 7,900\n",
      "868,193 2.11 0.09 ii) Overseas - - - 0.00 - - - 0.00 0.00 b) Individuals i) Individual shareholders 4,425,469 1,754,140\n",
      "6,179,609 15.01 4,343,826 1,748,240 6,092,066 14.80 -0.21 holding  nominal share capital upto Rs.1 lakhs IV.\n",
      "SHAREHOLDING PATTERN (Equity Share capital Break up as % to total Equity) i) Category wise shareholding Category of No.\n",
      "of Shares held at the beginning No. of Shares held at the end %change Shareholders of the year of the year during Demat\n",
      "Physical Total % of Demat Physical Total % of the year Total Total Shares Share\n",
      "\n",
      "head disqualifiers: []\n",
      "tail bleed indicators: []\n"
     ]
    }
   ],
   "source": [
    "# Targeted quality spot-checks (wrong-start + tail bleed)\n",
    "import textwrap\n",
    "\n",
    "TARGETS = [\n",
    "    (\"ALok\", \"5210700317\"),\n",
    "    (\"Amit_spinning\", \"5210760315\"),\n",
    "]\n",
    "\n",
    "for company_folder, token in TARGETS:\n",
    "    sub = results_df[(results_df[\"company_folder\"] == company_folder) & (results_df[\"report_file\"].str.contains(token, na=False))]\n",
    "    print(\"\\n===\", company_folder, token, \"===\")\n",
    "    if sub.empty:\n",
    "        print(\"No row found\")\n",
    "        continue\n",
    "\n",
    "    row = sub.iloc[0]\n",
    "    print(\n",
    "        \"file=\", row[\"report_file\"],\n",
    "        \"year=\", row[\"financial_year\"],\n",
    "        \"start=\", row[\"mdna_start_page\"],\n",
    "        \"end=\", row[\"mdna_end_page\"],\n",
    "        \"start_method=\", row[\"start_method\"],\n",
    "        \"end_method=\", row[\"end_method\"],\n",
    "        \"word_count=\", row.get(\"word_count\"),\n",
    "        \"quality_passed=\", row.get(\"quality_passed\"),\n",
    "    )\n",
    "\n",
    "    txt = row[\"mdna_text\"] or \"\"\n",
    "    print(\"text_len=\", len(txt))\n",
    "\n",
    "    head = txt[:900].replace(\"\\u00a0\", \" \")\n",
    "    tail = txt[-900:].replace(\"\\u00a0\", \" \")\n",
    "\n",
    "    print(\"\\n-- head (first ~900 chars) --\")\n",
    "    print(textwrap.fill(head, width=120))\n",
    "\n",
    "    print(\"\\n-- tail (last ~900 chars) --\")\n",
    "    print(textwrap.fill(tail, width=120))\n",
    "\n",
    "    # Quick indicators for the two bugs\n",
    "    head_flags = [\n",
    "        \"DIRECTORS' REPORT\",\n",
    "        \"DIRECTORS’ REPORT\",\n",
    "        \"BOARD'S REPORT\",\n",
    "        \"NOTICE\",\n",
    "        \"REPORT OF THE DIRECTORS\",\n",
    "    ]\n",
    "    bleed_flags = [\n",
    "        \"INDEPENDENT AUDITOR\",\n",
    "        \"AUDITOR\",\n",
    "        \"FINANCIAL STATEMENTS\",\n",
    "        \"CORPORATE GOVERNANCE\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\nhead disqualifiers:\", [f for f in head_flags if re.search(re.escape(f), txt[:2000], re.IGNORECASE)])\n",
    "\n",
    "    tail_bleeds = [f for f in bleed_flags if re.search(re.escape(f), txt[-2000:], re.IGNORECASE)]\n",
    "    print(\"tail bleed indicators:\", tail_bleeds)\n",
    "    for f in tail_bleeds:\n",
    "        print(\"  -\", f, \"first_idx=\", txt.lower().find(f.lower()), \"last_idx=\", txt.lower().rfind(f.lower()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
